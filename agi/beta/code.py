# agi

```json
{
    "files": [
        {
            "filename": "main.py",
            "content": "#!/usr/bin/env python3\n\"\"\"\nAGI System - Main Entry Point\n\nA basic implementation of an Artificial General Intelligence system\nusing OpenAI's API as the core reasoning engine.\n\"\"\"\n\nimport os\nimport sys\nfrom typing import List, Dict, Any\nfrom dotenv import load_dotenv\nfrom agi_core import AGICore\nfrom task_manager import TaskManager\nfrom memory_manager import MemoryManager\n\n# Load environment variables\nload_dotenv()\n\nclass AGISystem:\n    \"\"\"\n    Main AGI system class that coordinates all components.\n    \"\"\"\n    \n    def __init__(self, model: str = \"gpt-4\"):\n        \"\"\"\n        Initialize the AGI system.\n        \n        Args:\n            model: OpenAI model to use (default: \"gpt-4\")\n        \"\"\"\n        self.api_key = os.getenv(\"OPENAI_API_KEY\")\n        if not self.api_key:\n            print(\"Error: OPENAI_API_KEY not found in environment variables\")\n            print(\"Please create a .env file with OPENAI_API_KEY=your_key_here\")\n            sys.exit(1)\n            \n        # Initialize core components\n        self.core = AGICore(api_key=self.api_key, model=model)\n        self.task_manager = TaskManager()\n        self.memory = MemoryManager()\n        \n        # System prompt\n        self.system_prompt = \"\"\"You are an AGI (Artificial General Intelligence) system. \n        Your goal is to understand and solve complex problems by reasoning step by step.\n        You have access to various capabilities including memory, task management,\n        and reasoning. Always provide clear, thoughtful responses.\"\"\"\n        \n        print(f\"AGI System initialized with model: {model}\")\n        print(\"Type 'exit', 'quit', or 'bye' to end the conversation\\n\")\n    \n    def process_input(self, user_input: str) -> str:\n        \"\"\"\n        Process user input and generate response.\n        \"\"\"\n        # Store input in memory\n        self.memory.add_interaction(\"user\", user_input)\n        \n        # Check for exit commands\n        if user_input.lower() in ['exit', 'quit', 'bye']:\n            return \"Goodbye!\"\n        \n        # Get conversation history\n        history = self.memory.get_recent_interactions(5)\n        \n        # Prepare messages for the model\n        messages = [\n            {\"role\": \"system\", \"content\": self.system_prompt},\n        ]\n        \n        # Add history\n        for role, content in history:\n            messages.append({\"role\": role, \"content\": content})\n        \n        # Add current input\n        messages.append({\"role\": \"user\", \"content\": user_input})\n        \n        try:\n            # Get response from core reasoning engine\n            response = self.core.reason(messages)\n            \n            # Store response in memory\n            self.memory.add_interaction(\"assistant\", response)\n            \n            # Check if a task should be created\n            if self._should_create_task(user_input):\n                task_id = self.task_manager.create_task(user_input)\n                response += f\"\\n\\n[Task created with ID: {task_id}]\"\n            \n            return response\n            \n        except Exception as e:\n            return f\"Error: {str(e)}\"\n    \n    def _should_create_task(self, input_text: str) -> bool:\n        \"\"\"\n        Simple heuristic to determine if input should become a task.\n        \"\"\"\n        task_keywords = ['todo', 'task', 'remind', 'remember to', 'need to', 'should']\n        return any(keyword in input_text.lower() for keyword in task_keywords)\n    \n    def interactive_mode(self):\n        \"\"\"\n        Run AGI in interactive mode.\n        \"\"\"\n        print(\"AGI System Ready. Start chatting!\\n\")\n        \n        while True:\n            try:\n                user_input = input(\"You: \").strip()\n                \n                if not user_input:\n                    continue\n                    \n                if user_input.lower() in ['exit', 'quit', 'bye']:\n                    print(\"AGI: Goodbye!\")\n                    break\n                \n                response = self.process_input(user_input)\n                print(f\"AGI: {response}\\n\")\n                \n            except KeyboardInterrupt:\n                print(\"\\n\\nGoodbye!\")\n                break\n            except Exception as e:\n                print(f\"Error: {str(e)}\")\n\n\ndef main():\n    \"\"\"Main function to run the AGI system.\"\"\"\n    # Initialize the system\n    agi = AGISystem(model=\"gpt-4\")\n    \n    # Run in interactive mode\n    agi.interactive_mode()\n\n\nif __name__ == \"__main__\":\n    main()\n"
        },
        {
            "filename": "agi_core.py",
            "content": "\"\"\"\nAGI Core - Reasoning Engine\n\nCore reasoning component that interfaces with OpenAI's API.\n\"\"\"\n\nimport openai\nfrom typing import List, Dict, Any\nimport json\n\n\nclass AGICore:\n    \"\"\"\n    Core reasoning engine using OpenAI's API.\n    \"\"\"\n    \n    def __init__(self, api_key: str, model: str = \"gpt-4\", temperature: float = 0.7):\n        \"\"\"\n        Initialize the reasoning engine.\n        \n        Args:\n            api_key: OpenAI API key\n            model: Model to use for reasoning\n            temperature: Controls randomness (0.0 to 1.0)\n        \"\"\"\n        openai.api_key = api_key\n        self.model = model\n        self.temperature = temperature\n        \n        # Default parameters\n        self.max_tokens = 2000\n        self.top_p = 0.9\n        self.frequency_penalty = 0.0\n        self.presence_penalty = 0.0\n    \n    def reason(self, messages: List[Dict[str, str]]) -> str:\n        \"\"\"\n        Perform reasoning based on conversation history.\n        \n        Args:\n            messages: List of message dictionaries with 'role' and 'content'\n            \n        Returns:\n            Generated response\n        \"\"\"\n        try:\n            response = openai.ChatCompletion.create(\n                model=self.model,\n                messages=messages,\n                temperature=self.temperature,\n                max_tokens=self.max_tokens,\n                top_p=self.top_p,\n                frequency_penalty=self.frequency_penalty,\n                presence_penalty=self.presence_penalty\n            )\n            \n            return response.choices[0].message.content.strip()\n            \n        except openai.error.AuthenticationError:\n            return \"Error: Invalid API key. Please check your OPENAI_API_KEY.\"\n        except openai.error.RateLimitError:\n            return \"Error: Rate limit exceeded. Please try again later.\"\n        except openai.error.APIError as e:\n            return f\"API Error: {str(e)}\"\n        except Exception as e:\n            return f\"Unexpected error: {str(e)}\"\n    \n    def analyze_task(self, task_description: str) -> Dict[str, Any]:\n        \"\"\"\n        Analyze a task and break it down into steps.\n        \n        Args:\n            task_description: Description of the task\n            \n        Returns:\n            Dictionary with task analysis\n        \"\"\"\n        analysis_prompt = f\"\"\"\n        Analyze the following task and provide:\n        1. Complexity level (Low/Medium/High)\n        2. Estimated time to complete\n        3. Required resources\n        4. Prerequisites\n        5. Potential challenges\n        \n        Task: {task_description}\n        \n        Provide response as JSON format with keys: complexity, estimated_time, resources, prerequisites, challenges.\n        \"\"\"\n        \n        messages = [\n            {\"role\": \"system\", \"content\": \"You are a task analysis expert.\"},\n            {\"role\": \"user\", \"content\": analysis_prompt}\n        ]\n        \n        response = self.reason(messages)\n        \n        # Try to parse JSON response\n        try:\n            # Find JSON in response (in case there's additional text)\n            start_idx = response.find('{')\n            end_idx = response.rfind('}') + 1\n            json_str = response[start_idx:end_idx]\n            return json.loads(json_str)\n        except:\n            # Return as plain text if JSON parsing fails\n            return {\"analysis\": response}\n"
        },
        {
            "filename": "memory_manager.py",
            "content": "\"\"\"\nMemory Manager - Long-term and short-term memory storage\n\nHandles storage and retrieval of conversation history and knowledge.\n\"\"\"\n\nimport json\nimport time\nfrom datetime import datetime\nfrom typing import List, Tuple, Dict, Any\nimport pickle\nimport os\n\n\nclass MemoryManager:\n    \"\"\"\n    Manages both short-term and long-term memory for the AGI system.\n    \"\"\"\n    \n    def __init__(self, memory_file: str = \"memory.json\"):\n        \"\"\"\n        Initialize memory manager.\n        \n        Args:\n            memory_file: File to store memory data\n        \"\"\"\n        self.memory_file = memory_file\n        self.short_term_memory = []  # Recent interactions\n        self.long_term_memory = []   # Important facts and learnings\n        \n        # Load existing memory if available\n        self._load_memory()\n    \n    def _load_memory(self):\n        \"\"\"Load memory from file.\"\"\"\n        try:\n            if os.path.exists(self.memory_file):\n                with open(self.memory_file, 'r') as f:\n                    data = json.load(f)\n                    self.short_term_memory = data.get('short_term', [])\n                    self.long_term_memory = data.get('long_term', [])\n                print(f\"Loaded memory with {len(self.short_term_memory)} short-term and {len(self.long_term_memory)} long-term items\")\n        except Exception as e:\n            print(f\"Could not load memory: {str(e)}\")\n    \n    def _save_memory(self):\n        \"\"\"Save memory to file.\"\"\"\n        try:\n            data = {\n                'short_term': self.short_term_memory,\n                'long_term': self.long_term_memory,\n                'last_updated': datetime.now().isoformat()\n            }\n            with open(self.memory_file, 'w') as f:\n                json.dump(data, f, indent=2)\n        except Exception as e:\n            print(f\"Could not save memory: {str(e)}\")\n    \n    def add_interaction(self, role: str, content: str):\n        \"\"\"\n        Add an interaction to short-term memory.\n        \n        Args:\n            role: 'user' or 'assistant'\n            content: The message content\n        \"\"\"\n        timestamp = datetime.now().isoformat()\n        interaction = {\n            'timestamp': timestamp,\n            'role': role,\n            'content': content\n        }\n        \n        self.short_term_memory.append(interaction)\n        \n        # Keep only last 100 interactions in short-term memory\n        if len(self.short_term_memory) > 100:\n            self.short_term_memory = self.short_term_memory[-100:]\n        \n        # Auto-save\n        self._save_memory()\n    \n    def add_to_long_term(self, fact: str, category: str = \"general\"):\n        \"\"\"\n        Add a fact to long-term memory.\n        \n        Args:\n            fact: The fact to remember\n            category: Category for organization\n        \"\"\"\n        timestamp = datetime.now().isoformat()\n        memory_item = {\n            'timestamp': timestamp,\n            'fact': fact,\n            'category': category,\n            'access_count': 0\n        }\n        \n        self.long_term_memory.append(memory_item)\n        self._save_memory()\n    \n    def get_recent_interactions(self, n: int = 10) -> List[Tuple[str, str]]:\n        \"\"\"\n        Get recent interactions.\n        \n        Args:\n            n: Number of interactions to retrieve\n            \n        Returns:\n            List of (role, content) tuples\n        \"\"\"\n        recent = self.short_term_memory[-n:]\n        return [(item['role'], item['content']) for item in recent]\n    \n    def search_memory(self, query: str, limit: int = 5) -> List[Dict[str, Any]]:\n        \"\"\"\n        Search both short-term and long-term memory.\n        \n        Args:\n            query: Search query\n            limit: Maximum number of results\n            \n        Returns:\n            List of relevant memory items\n        \"\"\"\n        results = []\n        \n        # Search short-term memory\n        for item in reversed(self.short_term_memory):\n            if query.lower() in item['content'].lower():\n                results.append({\n                    'type': 'short_term',\n                    'content': item['content'],\n                    'role': item['role'],\n                    'timestamp': item['timestamp']\n                })\n                if len(results) >= limit:\n                    break\n        \n        # Search long-term memory\n        for item in self.long_term_memory:\n            if query.lower() in item['fact'].lower():\n                # Update access count\n                item['access_count'] = item.get('access_count', 0) + 1\n                \n                results.append({\n                    'type': 'long_term',\n                    'content': item['fact'],\n                    'category': item['category'],\n                    'timestamp': item['timestamp'],\n                    'access_count': item['access_count']\n                })\n                if len(results) >= limit:\n                    break\n        \n        self._save_memory()\n        return results\n    \n    def clear_short_term_memory(self):\n        \"\"\"Clear short-term memory.\"\"\"\n        self.short_term_memory = []\n        self._save_memory()\n        print(\"Short-term memory cleared\")\n    \n    def get_statistics(self) -> Dict[str, Any]:\n        \"\"\"Get memory statistics.\"\"\"\n        return {\n            'short_term_count': len(self.short_term_memory),\n            'long_term_count': len(self.long_term_m