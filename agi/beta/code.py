# agi

{
    "files": [
        {
            "filename": "app.py",
            "content": "#!/usr/bin/env python3\n\"\"\"Main entry point for the AGI micro‑service.\n\nThe service exposes a simple HTTP JSON API that forwards user prompts to the\nOpenAI Chat Completion endpoint and returns the model's response.\n\nIt is deliberately lightweight – the goal is to provide a clean, testable\nfoundation that can be extended into a more sophisticated AGI‑like system.\n\"\"\"\n\nimport logging\nfrom flask import Flask, request, jsonify\nfrom openai_client import OpenAIChatService\nfrom config import Settings\n\n# ---------------------------------------------------------------------------\n# Application setup\n# ---------------------------------------------------------------------------\n\ndef create_app() -> Flask:\n    \"\"\"Factory to create a Flask application.\n\n    Returns\n    -------\n    Flask\n        Configured Flask instance.\n    \"\"\"\n    app = Flask(__name__)\n    # Load configuration from environment / .env\n    app.config.from_object(Settings())\n\n    # Initialise logging – Flask uses the root logger by default.\n    logging.basicConfig(\n        level=app.config[\"LOG_LEVEL\"],\n        format=\"[%(asctime)s] %(levelname)s in %(module)s: %(message)s\",\n    )\n    logger = logging.getLogger(__name__)\n    logger.info(\"AGI service starting up\")\n\n    # Initialise the OpenAI wrapper – single instance is enough for our use‑case.\n    chat_service = OpenAIChatService(api_key=app.config[\"OPENAI_API_KEY\"],\n                                    model=app.config[\"OPENAI_MODEL\"])\n\n    # -----------------------------------------------------------------------\n    # Routes\n    # -----------------------------------------------------------------------\n    @app.route(\"/health\", methods=[\"GET\"])\n    def health_check():\n        \"\"\"Simple health check endpoint used by orchestration tools.\"\"\"\n        return jsonify({\"status\": \"ok\"})\n\n    @app.route(\"/chat\", methods=[\"POST\"])\n    def chat():\n        \"\"\"Chat endpoint.\n\n        Expected JSON payload::\n\n            {\n                \"messages\": [\n                    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n                    {\"role\": \"user\", \"content\": \"Hello!\"}\n                ]\n            }\n\n        Returns the model's reply in the same message format.\n        \"\"\"\n        payload = request.get_json(force=True)\n        if not payload or \"messages\" not in payload:\n            logger.warning(\"Invalid request payload: %s\", payload)\n            return (\n                jsonify({\"error\": \"'messages' field is required\"}),\n                400,\n            )\n        try:\n            response = chat_service.get_completion(messages=payload[\"messages\"])\n            return jsonify({\"reply\": response})\n        except Exception as exc:\n            logger.exception(\"Error while processing chat request\")\n            return (\n                jsonify({\"error\": str(exc)}),\n                500,\n            )\n\n    return app\n\n\nif __name__ == \"__main__\":\n    # Running directly is convenient for local development.\n    application = create_app()\n    application.run(host=\"0.0.0.0\", port=8000, debug=False)\n"
        },
        {
            "filename": "openai_client.py",
            "content": "#!/usr/bin/env python3\n\"\"\"Thin wrapper around the OpenAI Chat Completion API.\n\nThe wrapper isolates third‑party calls, making them easier to mock in unit tests\nand to replace with alternative providers in the future.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport json\nimport logging\nfrom typing import List, Dict, Any\n\nimport httpx\n\nlogger = logging.getLogger(__name__)\n\n\nclass OpenAIChatService:\n    \"\"\"Service class responsible for communicating with OpenAI's chat models.\n\n    Parameters\n    ----------\n    api_key: str\n        The secret key used for authentication.\n    model: str, optional\n        Model identifier, e.g. ``gpt-4o-mini``. Defaults to ``gpt-3.5-turbo``.\n    timeout: float, optional\n        Request timeout in seconds. Default is 30 seconds.\n    \"\"\"\n\n    def __init__(self, api_key: str, model: str = \"gpt-3.5-turbo\", timeout: float = 30.0):\n        if not api_key:\n            raise ValueError(\"OpenAI API key must be provided\")\n        self.api_key = api_key\n        self.model = model\n        self.base_url = \"https://api.openai.com/v1\"\n        self.timeout = timeout\n        self.client = httpx.AsyncClient(timeout=self.timeout)\n        logger.debug(\"OpenAIChatService initialised for model %s\", self.model)\n\n    async def _post(self, endpoint: str, json_body: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"Internal helper to issue a POST request to the OpenAI API.\n        \"\"\"\n        url = f\"{self.base_url}{endpoint}\"\n        headers = {\n            \"Authorization\": f\"Bearer {self.api_key}\",\n            \"Content-Type\": \"application/json\",\n        }\n        logger.debug(\"POST %s payload: %s\", url, json_body)\n        response = await self.client.post(url, headers=headers, json=json_body)\n        response.raise_for_status()\n        logger.debug(\"Response status %s\", response.status_code)\n        return response.json()\n\n    def get_completion(self, messages: List[Dict[str, str]]) -> str:\n        \"\"\"Synchronous façade that returns the assistant's reply.\n\n        This method is deliberately synchronous because Flask (the web framework\n        used in *app.py*) runs in a WSGI context. Internally we run the async\n        request using ``asyncio.run`` which is safe for short‑lived calls.\n        \"\"\"\n        import asyncio\n\n        async def _inner() -> str:\n            payload = {\n                \"model\": self.model,\n                \"messages\": messages,\n                \"temperature\": 0.7,\n            }\n            data = await self._post(\"/chat/completions\", payload)\n            # The API returns a list of choices – we take the first.\n            try:\n                reply = data[\"choices\"][0][\"message\"][\"content\"]\n                logger.info(\"Received completion from OpenAI\")\n                return reply.strip()\n            except (KeyError, IndexError) as exc:\n                logger.error(\"Malformed response from OpenAI: %s\", json.dumps(data))\n                raise RuntimeError(\"Invalid response structure from OpenAI\") from exc\n\n        return asyncio.run(_inner())\n\n    async def close(self) -> None:\n        \"\"\"Close the underlying HTTP client – should be called on shutdown.\"\"\"\n        await self.client.aclose()\n"
        },
        {
            "filename": "config.py",
            "content": "#!/usr/bin/env python3\n\"\"\"Configuration handling for the AGI service.\n\nValues are read from environment variables with sensible defaults. In a real\ndeployment you would likely use a .env file or a secret manager.\n\"\"\"\n\nimport os\nfrom dataclasses import dataclass\nfrom typing import Any\n\n\n@dataclass\nclass Settings:\n    \"\"\"Simple container for configuration values.\n\n    The Flask ``app.config.from_object`` method can consume any object with\n    uppercase attributes, which is why we keep the attribute names in upper\n    case.\n    \"\"\"\n\n    # Core OpenAI settings\n    OPENAI_API_KEY: str = os.getenv(\"OPENAI_API_KEY\", \"\")\n    OPENAI_MODEL: str = os.getenv(\"OPENAI_MODEL\", \"gpt-3.5-turbo\")\n\n    # Application behaviour\n    LOG_LEVEL: str = os.getenv(\"LOG_LEVEL\", \"INFO\")\n    DEBUG: bool = os.getenv(\"DEBUG\", \"false\").lower() == \"true\"\n\n    # Flask specific defaults – can be overridden by environment.\n   