# agi

```json
{
    "files": [
        {
            "filename": "requirements.txt",
            "content": "openai>=1.0.0\npython-dotenv>=1.0.0\nasyncio\nlogging\n"
        },
        {
            "filename": ".env.example",
            "content": "# OpenAI API Configuration\nOPENAI_API_KEY=your_api_key_here\nOPENAI_MODEL=gpt-4-turbo-preview\n\n# AGI Configuration\nAGI_NAME=AGI_Assistant\nMAX_MEMORY_SIZE=1000\nTEMPERATURE=0.7\n"
        },
        {
            "filename": "config.py",
            "content": "import os\nfrom dotenv import load_dotenv\n\n# Load environment variables\nload_dotenv()\n\nclass Config:\n    \"\"\"Configuration class for AGI system\"\"\"\n    \n    # OpenAI API settings\n    OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n    OPENAI_MODEL = os.getenv('OPENAI_MODEL', 'gpt-4-turbo-preview')\n    \n    # AGI settings\n    AGI_NAME = os.getenv('AGI_NAME', 'AGI_Assistant')\n    MAX_MEMORY_SIZE = int(os.getenv('MAX_MEMORY_SIZE', 1000))\n    TEMPERATURE = float(os.getenv('TEMPERATURE', 0.7))\n    \n    @classmethod\n    def validate(cls):\n        \"\"\"Validate required configuration\"\"\"\n        if not cls.OPENAI_API_KEY:\n            raise ValueError(\"OPENAI_API_KEY is required in environment variables\")\n        \n        return True\n"
        },
        {
            "filename": "memory.py",
            "content": "from typing import List, Dict, Any\nfrom collections import deque\nimport json\n\nclass MemorySystem:\n    \"\"\"Memory management system for AGI\"\"\"\n    \n    def __init__(self, max_size: int = 1000):\n        \"\"\"Initialize memory system\"\"\"\n        self.max_size = max_size\n        self.memory = deque(maxlen=max_size)\n        self.memory_index = {}\n        \n    def add(self, role: str, content: str, metadata: Dict[str, Any] = None):\n        \"\"\"Add a memory entry\"\"\"\n        memory_entry = {\n            'role': role,\n            'content': content,\n            'timestamp': self._get_timestamp(),\n            'metadata': metadata or {}\n        }\n        \n        self.memory.append(memory_entry)\n        self._update_index(memory_entry)\n        \n        return memory_entry\n    \n    def get_recent(self, n: int = 10) -> List[Dict[str, Any]]:\n        \"\"\"Get recent n memories\"\"\"\n        return list(self.memory)[-n:]\n    \n    def search(self, query: str, limit: int = 5) -> List[Dict[str, Any]]:\n        \"\"\"Search memories by content\"\"\"\n        results = []\n        query_lower = query.lower()\n        \n        for memory in reversed(self.memory):\n            if query_lower in memory['content'].lower():\n                results.append(memory)\n                if len(results) >= limit:\n                    break\n        \n        return results\n    \n    def clear(self):\n        \"\"\"Clear all memories\"\"\"\n        self.memory.clear()\n        self.memory_index.clear()\n    \n    def get_conversation_history(self) -> List[Dict[str, str]]:\n        \"\"\"Get conversation history in OpenAI format\"\"\"\n        history = []\n        for memory in self.memory:\n            if memory['role'] in ['user', 'assistant', 'system']:\n                history.append({\n                    'role': memory['role'],\n                    'content': memory['content']\n                })\n        return history\n    \n    def _get_timestamp(self) -> str:\n        \"\"\"Get current timestamp\"\"\"\n        from datetime import datetime\n        return datetime.now().isoformat()\n    \n    def _update_index(self, memory_entry: Dict[str, Any]):\n        \"\"\"Update search index\"\"\"\n        # Simple keyword indexing (can be enhanced)\n        words = memory_entry['content'].lower().split()\n        for word in words:\n            if len(word) > 3:  # Only index words longer than 3 characters\n                if word not in self.memory_index:\n                    self.memory_index[word] = []\n                self.memory_index[word].append(len(self.memory) - 1)\n    \n    def __len__(self) -> int:\n        \"\"\"Get number of memories\"\"\"\n        return len(self.memory)\n"
        },
        {
            "filename": "agi_core.py",
            "content": "import asyncio\nimport logging\nfrom typing import Dict, Any, Optional\nfrom openai import AsyncOpenAI\n\nfrom config import Config\nfrom memory import MemorySystem\n\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\nclass AGICore:\n    \"\"\"Core AGI system using OpenAI API\"\"\"\n    \n    def __init__(self):\n        \"\"\"Initialize AGI core\"\"\"\n        Config.validate()\n        \n        self.client = AsyncOpenAI(api_key=Config.OPENAI_API_KEY)\n        self.memory = MemorySystem(max_size=Config.MAX_MEMORY_SIZE)\n        self.name = Config.AGI_NAME\n        \n        # Initialize system prompt\n        self.system_prompt = f\"\"\"You are {self.name}, an Artificial General Intelligence (AGI). \n        You have the ability to reason, learn, and adapt to various tasks.\n        You have access to a memory system that stores past interactions.\n        Always respond thoughtfully and helpfully.\"\"\"\n        \n        self.memory.add('system', self.system_prompt)\n        \n    async def process(self, user_input: str) -> str:\n        \"\"\"Process user input and generate response\"\"\"\n        try:\n            # Add user input to memory\n            self.memory.add('user', user_input)\n            \n            # Get conversation history\n            messages = self.memory.get_conversation_history()\n            \n            # Call OpenAI API\n            response = await self._call_openai(messages)\n            \n            # Add assistant response to memory\n            self.memory.add('assistant', response)\n            \n            logger.info(f\"Processed input: {user_input[:50]}...\")\n            return response\n            \n        except Exception as e:\n            logger.error(f\"Error processing input: {e}\")\n            return f\"I encountered an error: {str(e)}\"\n    \n    async def _call_openai(self, messages: list) -> str:\n        \"\"\"Call OpenAI API with conversation history\"\"\"\n        try:\n            response = await self.client.chat.completions.create(\n                model=Config.OPENAI_MODEL,\n                messages=messages,\n                temperature=Config.TEMPERATURE,\n                max_tokens=2000\n            )\n            \n            return response.choices[0].message.content\n            \n        except Exception as e:\n            logger.error(f\"OpenAI API error: {e}\")\n            raise\n    \n    def get_memory_summary(self) -> Dict[str, Any]:\n        \"\"\"Get memory system summary\"\"\"\n        return {\n            'total_memories': len(self.memory),\n            'recent_memories': self.memory.get_recent(5),\n            'memory_size': Config.MAX_MEMORY_SIZE\n        }\n    \n    def clear_memory(self):\n        \"\"\"Clear all memories and reset system prompt\"\"\"\n        self.memory.clear()\n        self.memory.add('system', self.system_prompt)\n        logger.info(\"Memory cleared\")\n    \n    async def learn_from_context(self, context: str):\n        \"\"\"Learn from additional context\"\"\"\n        learning_prompt = f\"\"\"Based on this context, update your knowledge:\n        {context}\n        \n        Summarize what you've learned.\"\"\"\n        \n        self.memory.add('system', learning_prompt)\n        response = await self.process(\"Please summarize what you've learned from the context provided.\")\n        \n        # Remove the learning prompt to keep system prompt clean\n        self.memory.memory.pop()\n        \n        return response\n"
        },
        {
            "filename": "main.py",
            "content": "import asyncio\nimport sys\nfrom agi_core import AGICore\n\nasync def main():\n    \"\"\"Main entry point for AGI system\"\"\"\n    print(\"Initializing AGI System...\")\n    \n    # Initialize AGI\n    agi = AGICore()\n    \n    print(f\"AGI '{agi.name}' initialized successfully!\")\n    print(\"Type 'exit' to quit, 'clear' to clear memory, 'summary' for memory info\")\n    print(\"Type 'learn <context>' to provide learning context\")\n    print(\"-\" * 50)\n    \n    while True:\n        try:\n            # Get user input\n            user_input = input(\"\\nYou: \").strip()\n            \n            if not user_input:\n                continue\n                \n            # Handle special commands\n            if user_input.lower() == 'exit':\n                print(\"Goodbye!\")\n                break\n                \n            elif user_input.lower() == 'clear':\n                agi.clear_memory()\n                print(\"Memory cleared.\")\n                continue\n                \n            elif user_input.lower() == 'summary':\n                summary = agi.get_memory_summary()\n                print(f\"\\nMemory Summary:\")\n                print(f\"Total memories: {summary['total_memories']}\")\n                print(f\"Memory capacity: {summary['memory_size']}\")\n                continue\n                \n            elif user_input.lower().startswith('learn '):\n                context = user_input[6:]\n                if context:\n                    print(\"Learning from context...\")\n                    response = await agi.learn_from_context(context)\n                    print(f\"\\n{agi.name}: {response}\")\n                else:\n                    print(\"Please provide context after 'learn' command\")\n                continue\n            \n            # Process normal input\n            print(f\"\\n{agi.name} is thinking...\")\n            response = await agi.process(user_input)\n            print(f\"\\n{agi.name}: {response}\")\n            \n        except KeyboardInterrupt:\n            print(\"\\n\\nInterrupted. Goodbye!\")\n            break\n        except Exception as e:\n            print(f\"\\nError: {e}\")\n            continue\n\nif __name__ == \"__main__\":\n    if sys.version_info >= (3, 7):\n        asyncio.run(main())\n    else:\n        # For older Python versions\n        loop = asyncio.get_event_loop()\n        loop.run_until_complete(main())\n"
        },
        {
            "filename": "README.md",
            "content": "# AGI System\n\nAn implementation of an Artificial General Intelligence (AGI) system using OpenAI's API.\n\n## Features\n\n- **Memory System**: Persistent memory that stores conversation history and context\n- **Learning Capability**: Ability to learn from provided context\n- **Configurable**: Easy configuration via environment variables\n- **Asynchronous**: Built with async/await for efficient API calls\n\n## Setup\n\n1. **Install dependencies**:\n```bash\npip install -r requirements.txt\n```\n\n2. **Configure environment**:\n   - Copy `.env.example` to `.env`\n   - Add your OpenAI API key to `.env`\n\n3. **Run the AGI**:\n```bash\npython main.py\n```\n\n## Configuration\n\nEdit `.env` file to configure:\n- `OPENAI_API_KEY`: Your OpenAI API key (required)\n- `OPENAI_MODEL`: OpenAI model to use (default: gpt-4-turbo-preview)\n- `AGI_NAME`: Name of your AGI (default: AGI_Assistant)\n- `MAX_MEMORY_SIZE`: Maximum number of memories to store (default: 1000)\n- `TEMPERATURE`: Creativity temperature (default: 0.7)\n\n## Usage\n\nOnce running, you can:\n- Type normal messages to chat with the AGI\n- Type `learn <context>` to provide learning material\n- Type `clear` to clear memory\n- Type `summary` to see memory statistics\n- Type `exit` to quit\n\n## Architecture\n\n- `config.py`: Configuration management\n- `memory.py`: Memory system with search capabilities\n- `agi_core.py`: Core AGI logic with OpenAI integration\n- `main.py`: Command-line interface\n\n## Requirements\n\n- Python 3.7+\n- OpenAI API key\n- Internet connection for API calls\n\n## Notes\n\nThis is a basic implementation of AGI concepts. For production use, consider:\n- Adding more sophisticated memory management\n- Implementing file I/O for persistent storage\n- Adding error handling and retry logic\n- Implementing rate limiting\n- Adding security measures for API key management\n"
        },
        {
            "filename": "test_agi.py",
            "content": "import asyncio\nimport pytest\nfrom unittest.mock import AsyncMock, Mock, patch\nfrom agi_core import AGICore\nfrom config import Config\n\nclass TestAGICore:\n    \"\"\"Test cases for AGI Core\"\"\"\n    \n    @pytest.fixture\n    def mock_config(self):\n        \"\"\"Mock configuration\"\"\"\n        with patch.object(Config, 'OPENAI_API_KEY', 'test_key'):\n            with patch.object(Config, 'OPENAI_MODEL', 'gpt-4'):\n                with patch.object(Config, 'AGI_NAME', 'TestAGI'):\n                    with patch.object(Config, 'MAX_MEMORY_SIZE', 100):\n                        with patch.object(Config, 'TEMPERATURE', 0.7):\n                            yield\n    \n    @pytest.mark.asyncio\n    async def test_agi_initialization(self, mock_config):\n        \"\"\"Test AGI initialization\"\"\"\n        agi = AGICore()\n        assert agi.name == 'TestAGI'\n        assert len(agi.memory) > 0  # Should have system prompt\n        \n    @pytest.mark.asyncio\n    async def test_process_message(self, mock_config):\n        \"\"\"Test processing a message\"\"\"\n        agi = AGICore()\n        \n        # Mock OpenAI response\n        mock_response = Mock()\n        mock_response.choices = [Mock(message=Mock(content=\"Test response\"))]\n        \n        with patch.object(agi.client.chat.completions, 'create', \n                         AsyncMock(return_value=mock_response)):\n            response = await agi.process(\"Hello\")\n            \n            assert response == \"Test response\"\n            assert len(agi.memory) == 3  # System + User + Assistant\n            \n    def test_memory_management(self, mock_config):\n        \"\"\"Test memory system\"\"\"\n        agi = AGICore()\n        \n        # Test adding memory\n        agi.memory.add('user', 'Test message')\n        assert len(agi.memory) == 2  # System + User\n        \n        # Test getting recent\n        recent = agi.memory.get_recent(1)\n        assert len(recent) == 1\n        assert recent[0]['content'] == 'Test message'\n        \n        # Test clearing memory\n        agi.clear_memory()\n        assert len(agi.memory) == 1  # Only system prompt remains\n        \n    @pytest.mark.asyncio\n    async def test_learn_from_context(self, mock_config):\n        \"\"\"Test learning from context\"\"\"\n        agi = AGICore()\n        \n        # Mock OpenAI response for learning\n        mock_response = Mock()\n        mock_response.choices = [Mock(message=Mock(content=\"Learned summary\"))]\n        \n        with patch.object(agi.client.chat.completions, 'create', \n                         AsyncMock(return_value=mock_response)):\n            response = await agi.learn_from_context(\"Test learning context\")\n            \n            assert response == \"Learned summary\"\n\nif __name__ == \"__main__\":\n    # Run basic tests\n    print(\"Running basic tests...\")\n    \n    # Test configuration\n    Config.OPENAI_API_KEY = 'test'\n    Config.validate()\n    print(\"âœ“ Configuration test passed\")\n    \n    # Test memory system\n    from memory import MemorySystem\n    memory = MemorySystem(max_size=