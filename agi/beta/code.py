# agi

```json
{
    "files": [
        {
            "filename": "requirements.txt",
            "content": "openai>=1.0.0\npython-dotenv>=1.0.0\ntqdm>=4.66.0\npydantic>=2.0.0\nasyncio\njson5>=1.0.0\nrequests>=2.31.0\nnumpy>=1.24.0\npandas>=2.0.0\nmatplotlib>=3.7.0"
        },
        {
            "filename": ".env.example",
            "content": "# OpenAI API Configuration\nOPENAI_API_KEY=your_openai_api_key_here\n\n# Agent Configuration\nAGENT_NAME=AGI_Agent\nDEFAULT_MODEL=gpt-4-turbo-preview\nTEMPERATURE=0.7\nMAX_TOKENS=2000\n\n# System Settings\nLOG_LEVEL=INFO\nENABLE_MEMORY=true\nMEMORY_SIZE=1000\n\n# Project Paths\nDATA_DIR=./data\nLOGS_DIR=./logs\nMODELS_DIR=./models"
        },
        {
            "filename": "config.py",
            "content": "#!/usr/bin/env python3\n\"\"\"\nConfiguration management for the AGI project.\nLoads environment variables and provides configuration settings.\n\"\"\"\n\nimport os\nfrom pathlib import Path\nfrom typing import Optional\nfrom dotenv import load_dotenv\nfrom pydantic import BaseSettings, Field\n\n# Load environment variables from .env file\nload_dotenv()\n\n\nclass Settings(BaseSettings):\n    \"\"\"Application settings.\"\"\"\n    \n    # OpenAI API Configuration\n    openai_api_key: str = Field(..., env=\"OPENAI_API_KEY\")\n    default_model: str = Field(\"gpt-4-turbo-preview\", env=\"DEFAULT_MODEL\")\n    temperature: float = Field(0.7, env=\"TEMPERATURE\")\n    max_tokens: int = Field(2000, env=\"MAX_TOKENS\")\n    \n    # Agent Configuration\n    agent_name: str = Field(\"AGI_Agent\", env=\"AGENT_NAME\")\n    enable_memory: bool = Field(True, env=\"ENABLE_MEMORY\")\n    memory_size: int = Field(1000, env=\"MEMORY_SIZE\")\n    \n    # System Settings\n    log_level: str = Field(\"INFO\", env=\"LOG_LEVEL\")\n    data_dir: Path = Field(Path(\"./data\"), env=\"DATA_DIR\")\n    logs_dir: Path = Field(Path(\"./logs\"), env=\"LOGS_DIR\")\n    models_dir: Path = Field(Path(\"./models\"), env=\"MODELS_DIR\")\n    \n    class Config:\n        env_file = \".env\"\n        env_file_encoding = \"utf-8\"\n    \n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n        # Create directories if they don't exist\n        self.data_dir.mkdir(exist_ok=True)\n        self.logs_dir.mkdir(exist_ok=True)\n        self.models_dir.mkdir(exist_ok=True)\n\n\n# Global settings instance\nsettings = Settings()\n\n\nif __name__ == \"__main__\":\n    # Print current configuration (without API key)\n    config_dict = settings.dict()\n    config_dict[\"openai_api_key\"] = \"***REDACTED***\"\n    print(\"Current Configuration:\")\n    for key, value in config_dict.items():\n        print(f\"  {key}: {value}\")"
        },
        {
            "filename": "memory.py",
            "content": "#!/usr/bin/env python3\n\"\"\"\nMemory management for the AGI agent.\nImplements short-term and long-term memory mechanisms.\n\"\"\"\n\nimport json\nfrom datetime import datetime\nfrom pathlib import Path\nfrom typing import Dict, List, Optional, Any, Tuple\nfrom dataclasses import dataclass, asdict\nimport numpy as np\n\nfrom config import settings\n\n\n@dataclass\nclass MemoryItem:\n    \"\"\"A single memory item.\"\"\"\n    id: str\n    content: str\n    timestamp: datetime\n    metadata: Dict[str, Any]\n    importance: float = 0.5  # 0.0 to 1.0\n    \n    def to_dict(self) -> Dict[str, Any]:\n        \"\"\"Convert to dictionary.\"\"\"\n        data = asdict(self)\n        data[\"timestamp\"] = self.timestamp.isoformat()\n        return data\n    \n    @classmethod\n    def from_dict(cls, data: Dict[str, Any]) -> \"MemoryItem\":\n        \"\"\"Create from dictionary.\"\"\"\n        data = data.copy()\n        data[\"timestamp\"] = datetime.fromisoformat(data[\"timestamp\"])\n        return cls(**data)\n\n\nclass Memory:\n    \"\"\"Memory system for the AGI agent.\"\"\"\n    \n    def __init__(self, max_size: int = None):\n        \"\"\"Initialize memory system.\"\"\"\n        self.max_size = max_size or settings.memory_size\n        self.short_term_memory: List[MemoryItem] = []\n        self.long_term_memory: List[MemoryItem] = []\n        self.memory_file = settings.data_dir / \"memory.json\"\n        self.load_memory()\n    \n    def add(self, content: str, metadata: Optional[Dict[str, Any]] = None, \n            importance: float = 0.5) -> str:\n        \"\"\"\n        Add a new memory item.\n        \n        Args:\n            content: The content to remember\n            metadata: Additional metadata\n            importance: Importance score (0.0 to 1.0)\n        \n        Returns:\n            Memory ID\n        \"\"\"\n        if metadata is None:\n            metadata = {}\n        \n        memory_id = f\"mem_{datetime.now().timestamp()}_{len(self.short_term_memory)}\"\n        item = MemoryItem(\n            id=memory_id,\n            content=content,\n            timestamp=datetime.now(),\n            metadata=metadata,\n            importance=importance\n        )\n        \n        self.short_term_memory.append(item)\n        \n        # If important enough, add to long-term memory\n        if importance > 0.7:\n            self.long_term_memory.append(item)\n        \n        # Enforce memory limits\n        self._enforce_limits()\n        \n        return memory_id\n    \n    def retrieve(self, query: str, limit: int = 10) -> List[MemoryItem]:\n        \"\"\"\n        Retrieve relevant memories based on query.\n        \n        Args:\n            query: Search query\n            limit: Maximum number of memories to return\n        \n        Returns:\n            List of relevant memory items\n        \"\"\"\n        # Simple keyword matching (can be enhanced with embeddings)\n        query_lower = query.lower()\n        relevant_memories = []\n        \n        # Search in both short-term and long-term memory\n        all_memories = self.short_term_memory + self.long_term_memory\n        \n        for memory in all_memories:\n            if query_lower in memory.content.lower():\n                relevant_memories.append(memory)\n            elif any(query_lower in str(val).lower() for val in memory.metadata.values()):\n                relevant_memories.append(memory)\n        \n        # Sort by importance and recency\n        relevant_memories.sort(\n            key=lambda x: (x.importance, x.timestamp), \n            reverse=True\n        )\n        \n        return relevant_memories[:limit]\n    \n    def clear_short_term(self) -> None:\n        \"\"\"Clear short-term memory.\"\"\"\n        # Save important items to long-term memory first\n        for item in self.short_term_memory:\n            if item.importance > 0.7:\n                self.long_term_memory.append(item)\n        self.short_term_memory = []\n    \n    def save_memory(self) -> None:\n        \"\"\"Save memory to disk.\"\"\"\n        data = {\n            \"short_term\": [item.to_dict() for item in self.short_term_memory],\n            \"long_term\": [item.to_dict() for item in self.long_term_memory]\n        }\n        \n        with open(self.memory_file, \"w\") as f:\n            json.dump(data, f, indent=2, default=str)\n    \n    def load_memory(self) -> None:\n        \"\"\"Load memory from disk.\"\"\"\n        if self.memory_file.exists():\n            try:\n                with open(self.memory_file, \"r\") as f:\n                    data = json.load(f)\n                \n                self.short_term_memory = [\n                    MemoryItem.from_dict(item) for item in data.get(\"short_term\", [])\n                ]\n                self.long_term_memory = [\n                    MemoryItem.from_dict(item) for item in data.get(\"long_term\", [])\n                ]\n            except Exception as e:\n                print(f\"Error loading memory: {e}\")\n    \n    def _enforce_limits(self) -> None:\n        \"\"\"Enforce memory size limits.\"\"\"\n        if len(self.short_term_memory) > self.max_size:\n            # Remove oldest, least important memories\n            self.short_term_memory.sort(\n                key=lambda x: (x.importance, x.timestamp)\n            )\n            self.short_term_memory = self.short_term_memory[-self.max_size:]\n        \n        if len(self.long_term_memory) > self.max_size * 2:\n            self.long_term_memory.sort(\n                key=lambda x: (x.importance, x.timestamp)\n            )\n            self.long_term_memory = self.long_term_memory[-self.max_size * 2:]\n    \n    def get_stats(self) -> Dict[str, Any]:\n        \"\"\"Get memory statistics.\"\"\"\n        return {\n            \"short_term_count\": len(self.short_term_memory),\n            \"long_term_count\": len(self.long_term_memory),\n            \"max_size\": self.max_size,\n            \"total_count\": len(self.short_term_memory) + len(self.long_term_memory)\n        }\n\n\nif __name__ == \"__main__\":\n    # Test memory system\n    memory = Memory(max_size=5)\n    \n    # Add some test memories\n    memory.add(\"I learned about machine learning today.\", {\"topic\": \"ML\"}, 0.8)\n    memory.add(\"The weather is sunny today.\", {\"context\": \"weather\"}, 0.3)\n    memory.add(\"Python is a great programming language.\", {\"topic\": \"programming\"}, 0.9)\n    \n    # Retrieve memories\n    results = memory.retrieve(\"learning\")\n    print(f\"Found {len(results)} memories about 'learning':\")\n    for mem in results:\n        print(f\"  - {mem.content}\")\n    \n    # Save and load\n    memory.save_memory()\n    print(f\"Memory stats: {memory.get_stats()}\")"
        },
        {
            "filename": "agent.py",
            "content": "#!/usr/bin/env python3\n\"\"\"\nCore AGI agent implementation.\nHandles interactions with OpenAI API and manages agent state.\n\"\"\"\n\nimport json\nimport asyncio\nfrom datetime import datetime\nfrom typing import Dict, List, Optional, Any, Callable, Union\nfrom dataclasses import dataclass, asdict\nimport logging\n\nfrom openai import OpenAI, AsyncOpenAI\n\nfrom config import settings\nfrom memory import Memory\n\n\n# Configure logging\nlogging.basicConfig(\n    level=getattr(logging, settings.log_level),\n    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n)\nlogger = logging.getLogger(__name__)\n\n\n@dataclass\nclass Message:\n    \"\"\"A message in the conversation.\"\"\"\n    role: str  # \"system\", \"user\", \"assistant\", \"function\"\n    content: str\n    name: Optional[str] = None\n    function_call: Optional[Dict[str, Any]] = None\n    \n    def to_dict(self) -> Dict[str, Any]:\n        \"\"\"Convert to OpenAI API format.\"\"\"\n        data = {\n            \"role\": self.role,\n            \"content\": self.content\n        }\n        if self.name:\n            data[\"name\"] = self.name\n        if self.function_call:\n            data[\"function_call\"] = self.function_call\n        return data\n\n\n@dataclass\nclass AgentState:\n    \"\"\"Current state of the agent.\"\"\"\n    name: str\n    model: str\n    temperature: float\n    max_tokens: int\n    messages: List[Message]\n    functions: List[Dict[str, Any]]\n    memory: Optional[Memory] = None\n    \n    def to_dict(self) -> Dict[str, Any]:\n        \"\"\"Convert to dictionary for serialization.\"\"\"\n        return {\n            \"name\": self.name,\n            \"model\": self.model,\n            \"temperature\": self.temperature,\n            \"max_tokens\": self.max_tokens,\n            \"messages\": [msg.to_dict() for msg in self.messages],\n            \"functions\": self.functions,\n            \"has_memory\": self.memory is not None\n        }\n\n\nclass AGIAgent:\n    \"\"\"Main AGI agent class.\"\"\"\n    \n    def __init__(\n        self,\n        name: Optional[str] = None,\n        model: Optional[str] = None,\n        temperature: Optional[float] = None,\n        max_tokens: Optional[int] = None,\n        system_prompt: Optional[str] = None,\n        enable_memory: Optional[bool] = None\n    ):\n        \"\"\"\n        Initialize the AGI agent.\n        \n        Args:\n            name: Agent name\n            model: OpenAI model to use\n            temperature: Sampling temperature\n            max_tokens: Maximum tokens to generate\n            system_prompt: Initial system prompt\n            enable_memory: Whether to enable memory system\n        \"\"\"\n        self.name = name or settings.agent_name\n        self.model = model or settings.default_model\n        self.temperature = temperature or settings.temperature\n        self.max_tokens = max_tokens or settings.max_tokens\n        \n        # Initialize OpenAI clients\n        self.client = OpenAI(api_key=settings.openai_api_key)\n        self.async_client = AsyncOpenAI(api_key=settings.openai_api_key)\n        \n        # Initialize memory\n        self.enable_memory = enable_memory if enable_memory is not None else settings.enable_memory\n        self.memory = Memory() if self.enable_memory else None\n        \n        # Initialize conversation history\n        self.messages: List[Message] = []\n        \n        # Define available functions\n        self.functions = self._define_functions()\n        \n        # Set system prompt\n        default_system_prompt = \"\"\"You are an advanced AGI (Artificial General Intelligence) assistant. \nYou have the ability to reason, learn, and adapt to various tasks. \nYou should be helpful, accurate, and thoughtful in your responses.\nIf you don't know something, acknowledge it and offer to help find the information.\"\"\"\n        \n        system_message = Message(\n            role=\"system\",\n            content=system_prompt or default_system_prompt\n        )\n        self.messages.append(system_message)\n        \n        # Initialize agent state\n