# agi

```json
{
    "files": [
        {
            "filename": "requirements.txt",
            "content": "openai>=1.0.0\npython-dotenv>=1.0.0\nasyncio\npydantic>=2.0.0\nloguru>=0.7.0\nrich>=13.0.0"
        },
        {
            "filename": ".env.example",
            "content": "# OpenAI API Configuration\nOPENAI_API_KEY=your_openai_api_key_here\nOPENAI_BASE_URL=https://api.openai.com/v1\nOPENAI_MODEL=gpt-4-turbo-preview\n\n# AGI Configuration\nAGI_NAME=AGI_Assistant\nAGI_DESCRIPTION=A general artificial intelligence assistant\nMAX_MEMORY_SIZE=1000\nTEMPERATURE=0.7\nMAX_TOKENS=2000"
        },
        {
            "filename": "config.py",
            "content": "import os\nfrom typing import Optional\nfrom pydantic import BaseSettings, Field\nfrom dotenv import load_dotenv\n\n# Load environment variables\nload_dotenv()\n\n\nclass OpenAIConfig(BaseSettings):\n    \"\"\"OpenAI API configuration\"\"\"\n    api_key: str = Field(..., env=\"OPENAI_API_KEY\")\n    base_url: str = Field(\"https://api.openai.com/v1\", env=\"OPENAI_BASE_URL\")\n    model: str = Field(\"gpt-4-turbo-preview\", env=\"OPENAI_MODEL\")\n    temperature: float = Field(0.7, env=\"TEMPERATURE\")\n    max_tokens: int = Field(2000, env=\"MAX_TOKENS\")\n\n    class Config:\n        env_file = \".env\"\n\n\nclass AGIConfig(BaseSettings):\n    \"\"\"AGI system configuration\"\"\"\n    name: str = Field(\"AGI_Assistant\", env=\"AGI_NAME\")\n    description: str = Field(\n        \"A general artificial intelligence assistant\", \n        env=\"AGI_DESCRIPTION\"\n    )\n    max_memory_size: int = Field(1000, env=\"MAX_MEMORY_SIZE\")\n    enable_long_term_memory: bool = True\n    enable_self_reflection: bool = True\n    enable_learning: bool = True\n\n    class Config:\n        env_file = \".env\"\n\n\nclass Config:\n    \"\"\"Main configuration class\"\"\"\n    def __init__(self):\n        self.openai = OpenAIConfig()\n        self.agi = AGIConfig()\n\n\n# Global configuration instance\nconfig = Config()"
        },
        {
            "filename": "memory.py",
            "content": "from typing import List, Dict, Any, Optional\nfrom datetime import datetime\nfrom dataclasses import dataclass, asdict\nimport json\n\n\n@dataclass\nclass MemoryItem:\n    \"\"\"Individual memory item\"\"\"\n    id: str\n    content: str\n    timestamp: datetime\n    importance: float  # 0.0 to 1.0\n    category: str  # e.g., 'conversation', 'knowledge', 'reflection'\n    metadata: Dict[str, Any]\n\n    def to_dict(self) -> Dict[str, Any]:\n        \"\"\"Convert to dictionary\"\"\"\n        data = asdict(self)\n        data['timestamp'] = self.timestamp.isoformat()\n        return data\n\n    @classmethod\n    def from_dict(cls, data: Dict[str, Any]) -> 'MemoryItem':\n        \"\"\"Create from dictionary\"\"\"\n        data['timestamp'] = datetime.fromisoformat(data['timestamp'])\n        return cls(**data)\n\n\nclass MemorySystem:\n    \"\"\"Memory management system for AGI\"\"\"\n    \n    def __init__(self, max_size: int = 1000):\n        self.memories: List[MemoryItem] = []\n        self.max_size = max_size\n        self.next_id = 1\n    \n    def add_memory(self, content: str, importance: float = 0.5, \n                   category: str = \"conversation\", \n                   metadata: Optional[Dict[str, Any]] = None) -> str:\n        \"\"\"Add a new memory item\"\"\"\n        memory_id = f\"mem_{self.next_id}\"\n        self.next_id += 1\n        \n        item = MemoryItem(\n            id=memory_id,\n            content=content,\n            timestamp=datetime.now(),\n            importance=importance,\n            category=category,\n            metadata=metadata or {}\n        )\n        \n        self.memories.append(item)\n        self._enforce_size_limit()\n        \n        return memory_id\n    \n    def retrieve_relevant_memories(self, query: str, limit: int = 10) -> List[MemoryItem]:\n        \"\"\"Retrieve memories relevant to the query\"\"\"\n        # Simple keyword matching - can be enhanced with embeddings\n        query_lower = query.lower()\n        relevant = []\n        \n        for memory in self.memories:\n            if query_lower in memory.content.lower():\n                relevant.append(memory)\n            \n            if len(relevant) >= limit:\n                break\n        \n        # Sort by importance and recency\n        relevant.sort(key=lambda x: (x.importance, x.timestamp), reverse=True)\n        \n        return relevant\n    \n    def get_memory_by_id(self, memory_id: str) -> Optional[MemoryItem]:\n        \"\"\"Get a specific memory by ID\"\"\"\n        for memory in self.memories:\n            if memory.id == memory_id:\n                return memory\n        return None\n    \n    def clear_memory(self) -> None:\n        \"\"\"Clear all memories\"\"\"\n        self.memories = []\n    \n    def save_to_file(self, filepath: str) -> None:\n        \"\"\"Save memories to a file\"\"\"\n        data = {\n            'memories': [memory.to_dict() for memory in self.memories],\n            'next_id': self.next_id\n        }\n        \n        with open(filepath, 'w') as f:\n            json.dump(data, f, indent=2)\n    \n    def load_from_file(self, filepath: str) -> None:\n        \"\"\"Load memories from a file\"\"\"\n        try:\n            with open(filepath, 'r') as f:\n                data = json.load(f)\n            \n            self.memories = [MemoryItem.from_dict(mem) for mem in data['memories']]\n            self.next_id = data['next_id']\n        except FileNotFoundError:\n            print(f\"Memory file {filepath} not found, starting with empty memory\")\n    \n    def _enforce_size_limit(self) -> None:\n        \"\"\"Remove oldest, least important memories if over limit\"\"\"\n        if len(self.memories) > self.max_size:\n            # Sort by importance (ascending) and timestamp (ascending)\n            self.memories.sort(key=lambda x: (x.importance, x.timestamp))\n            # Remove excess memories\n            self.memories = self.memories[-self.max_size:]\n    \n    def get_statistics(self) -> Dict[str, Any]:\n        \"\"\"Get memory system statistics\"\"\"\n        return {\n            'total_memories': len(self.memories),\n            'max_size': self.max_size,\n            'categories': {\n                cat: len([m for m in self.memories if m.category == cat])\n                for cat in set(m.category for m in self.memories)\n            }\n        }"
        },
        {
            "filename": "openai_client.py",
            "content": "from typing import List, Dict, Any, Optional\nimport asyncio\nfrom openai import AsyncOpenAI\nfrom loguru import logger\n\nfrom config import config\n\n\nclass OpenAIClient:\n    \"\"\"Wrapper for OpenAI API calls\"\"\"\n    \n    def __init__(self):\n        self.client = AsyncOpenAI(\n            api_key=config.openai.api_key,\n            base_url=config.openai.base_url\n        )\n        self.model = config.openai.model\n        \n    async def chat_completion(self, messages: List[Dict[str, str]], \n                             temperature: Optional[float] = None,\n                             max_tokens: Optional[int] = None) -> str:\n        \"\"\"Get chat completion from OpenAI\"\"\"\n        try:\n            response = await self.client.chat.completions.create(\n                model=self.model,\n                messages=messages,\n                temperature=temperature or config.openai.temperature,\n                max_tokens=max_tokens or config.openai.max_tokens\n            )\n            \n            if response.choices and response.choices[0].message.content:\n                return response.choices[0].message.content.strip()\n            else:\n                logger.warning(\"No content in OpenAI response\")\n                return \"\"\n                \n        except Exception as e:\n            logger.error(f\"OpenAI API error: {e}\")\n            raise\n    \n    async def generate_embeddings(self, text: str) -> List[float]:\n        \"\"\"Generate embeddings for text\"\"\"\n        try:\n            response = await self.client.embeddings.create(\n                model=\"text-embedding-ada-002\",\n                input=text\n            )\n            return response.data[0].embedding\n        except Exception as e:\n            logger.error(f\"Embedding generation error: {e}\")\n            raise\n    \n    async def analyze_sentiment(self, text: str) -> Dict[str, Any]:\n        \"\"\"Analyze sentiment of text\"\"\"\n        prompt = f\"\"\"Analyze the sentiment of the following text and return a JSON with:\n        - sentiment: positive, negative, or neutral\n        - confidence: float between 0 and 1\n        - key_emotions: list of key emotions detected\n        \n        Text: {text}\n        \n        Return only valid JSON.\"\"\"\n        \n        response = await self.chat_completion([\n            {\"role\": \"system\", \"content\": \"You are a sentiment analysis assistant. Return only valid JSON.\"},\n            {\"role\": \"user\", \"content\": prompt}\n        ], temperature=0.1)\n        \n        try:\n            import json\n            return json.loads(response)\n        except:\n            return {\"sentiment\": \"neutral\", \"confidence\": 0.5, \"key_emotions\": []}\n    \n    async def summarize_text(self, text: str, max_length: int = 200) -> str:\n        \"\"\"Summarize text\"\"\"\n        prompt = f\"\"\"Summarize the following text in {max_length} characters or less:\n        \n        {text}\"\"\"\n        \n        return await self.chat_completion([\n            {\"role\": \"system\", \"content\": \"You are a summarization assistant.\"},\n            {\"role\": \"user\", \"content\": prompt}\n        ], temperature=0.3, max_tokens=max_length)"
        },
        {
            "filename": "agi_core.py",
            "content": "from typing import List, Dict, Any, Optional\nimport asyncio\nfrom datetime import datetime\nfrom loguru import logger\n\nfrom config import config\nfrom memory import MemorySystem\nfrom openai_client import OpenAIClient\n\n\nclass AGICore:\n    \"\"\"Core AGI system\"\"\"\n    \n    def __init__(self):\n        self.name = config.agi.name\n        self.description = config.agi.description\n        self.memory = MemorySystem(max_size=config.agi.max_memory_size)\n        self.openai_client = OpenAIClient()\n        self.conversation_history: List[Dict[str, str]] = []\n        self.reflection_count = 0\n        \n    async def process_input(self, user_input: str) -> str:\n        \"\"\"Process user input and generate response\"\"\"\n        logger.info(f\"Processing input: {user_input[:50]}...\")\n        \n        # Store user input in memory\n        self.memory.add_memory(\n            content=f\"User: {user_input}\",\n            category=\"conversation\",\n            importance=0.6\n        )\n        \n        # Retrieve relevant memories\n        relevant_memories = self.memory.retrieve_relevant_memories(user_input)\n        \n        # Build context from memories\n        memory_context = self._build_memory_context(relevant_memories)\n        \n        # Build system prompt\n        system_prompt = self._build_system_prompt(memory_context)\n        \n        # Add to conversation history\n        self.conversation_history.append({\"role\": \"user\", \"content\": user_input})\n        \n        # Prepare messages for OpenAI\n        messages = [\n            {\"role\": \"system\", \"content\": system_prompt}\n        ] + self.conversation_history[-10:]  # Keep last 10 messages for context\n        \n        # Get response from OpenAI\n        response = await self.openai_client.chat_completion(messages)\n        \n        # Store response in memory\n        self.memory.add_memory(\n            content=f\"{self.name}: {response}\",\n            category=\"conversation\",\n            importance=0.7\n        )\n        \n        # Add to conversation history\n        self.conversation_history.append({\"role\": \"assistant\", \"content\": response})\n        \n        # Perform self-reflection if enabled\n        if config.agi.enable_self_reflection:\n            await self._self_reflect(user_input, response)\n        \n        # Perform learning if enabled\n        if config.agi.enable_learning:\n            await self._learn_from_interaction(user_input, response)\n        \n        return response\n    \n    def _build_memory_context(self, memories: List[Any]) -> str:\n        \"\"\"Build context string from memories\"\"\"\n        if not memories:\n            return \"No relevant memories found.\"\n        \n        context_lines = [\"Relevant memories:\"]\n        for i, memory in enumerate(memories[:5], 1):  # Top 5 memories\n            timestamp = memory.timestamp.strftime(\"%Y-%m-%d %H:%M\")\n            context_lines.append(f\"{i}. [{timestamp}] {memory.content[:100]}...\")\n        \n        return \"\\n\".join(context_lines)\n    \n    def _build_system_prompt(self, memory_context: str) -> str:\n        \"\"\"Build the system prompt for OpenAI\"\"\"\n        prompt = f\"\"\"You are {self.name}, {self.description}.\n        \n        You have the following capabilities:\n        1. Memory: You can remember past conversations and information\n        2. Learning: You can learn from interactions\n        3. Self-reflection: You can reflect on your responses\n        \n        Current context:\n        {memory_context}\n        \n        Guidelines:\n        - Be helpful, accurate, and thoughtful\n        - Reference relevant memories when appropriate\n        - If you don't know something, say so\n        - Think step by step when solving complex problems\n        - Be concise but thorough\n        \n        Current time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\"\"\"\n        \n        return prompt\n    \n    async def _self_reflect(self, user_input: str, response: str) -> None:\n        \"\"\"Perform self-reflection on the interaction\"\"\"\n        self.reflection_count += 1\n        \n        # Only reflect every 5 interactions to save tokens\n        if self.reflection_count % 5 != 0:\n            return\n        \n        reflection_prompt = f\"\"\"Reflect on this interaction:\n        User: {user_input}\n        Your response: {response}\n        \n        Questions to consider:\n        1. Was the response helpful and accurate?\n        2. Could it have been improved?\n        3. What did you learn from this interaction?\n        4. How might this inform future responses?\"\"\"\n        \n        try:\n            reflection = await self.openai_client.chat_completion([\n                {\"role\": \"system\", \"content\": \"You are a self-reflection assistant. Provide thoughtful analysis.\"},\n                {\"role\": \"user\", \"content\": reflection_prompt}\n            ], temperature=0.5, max_tokens=300)\n            \n            # Store reflection in memory\n            self.memory.add_memory(\n                content=f\"Self-reflection: {reflection}\",\n                category=\"reflection\",\n                importance=0.8,\n                metadata={\n                    \"user_input\": user_input,\n                    \"original_response\": response,\n                    \"timestamp\": datetime.now().isoformat()\n                }\n            )\n            \n            logger.info(f\"Self-reflection completed and stored\"