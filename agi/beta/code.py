# agi

```json
{
    "files": [
        {
            "filename": "requirements.txt",
            "content": "openai>=1.0.0\npython-dotenv>=1.0.0\nasyncio>=3.4.3\npydantic>=2.0.0\nrich>=13.0.0\n"
        },
        {
            "filename": ".env.example",
            "content": "# OpenAI API Configuration\nOPENAI_API_KEY=your_api_key_here\nOPENAI_MODEL=gpt-4-turbo-preview\nOPENAI_BASE_URL=https://api.openai.com/v1\n\n# AGI Configuration\nAGI_NAME=AGI_Assistant\nAGI_MAX_MEMORY_SIZE=1000\nAGI_TEMPERATURE=0.7\n"
        },
        {
            "filename": "config.py",
            "content": "import os\nfrom typing import Optional\nfrom pydantic import BaseSettings, Field\n\n\nclass AGIConfig(BaseSettings):\n    \"\"\"AGI Configuration Settings\"\"\"\n    \n    # OpenAI API Settings\n    openai_api_key: str = Field(..., env=\"OPENAI_API_KEY\")\n    openai_model: str = Field(\"gpt-4-turbo-preview\", env=\"OPENAI_MODEL\")\n    openai_base_url: str = Field(\"https://api.openai.com/v1\", env=\"OPENAI_BASE_URL\")\n    \n    # AGI Settings\n    agi_name: str = Field(\"AGI_Assistant\", env=\"AGI_NAME\")\n    agi_max_memory_size: int = Field(1000, env=\"AGI_MAX_MEMORY_SIZE\")\n    agi_temperature: float = Field(0.7, env=\"AGI_TEMPERATURE\")\n    \n    class Config:\n        env_file = \".env\"\n        env_file_encoding = \"utf-8\"\n\n\nconfig = AGIConfig()\n"
        },
        {
            "filename": "memory.py",
            "content": "from typing import List, Dict, Any, Optional\nfrom datetime import datetime\nimport json\n\n\nclass MemoryItem:\n    \"\"\"Single memory item for AGI\"\"\"\n    \n    def __init__(self, content: str, metadata: Optional[Dict[str, Any]] = None):\n        self.content = content\n        self.metadata = metadata or {}\n        self.timestamp = datetime.now()\n        self.importance: float = 0.5  # Default importance score\n    \n    def to_dict(self) -> Dict[str, Any]:\n        \"\"\"Convert memory item to dictionary\"\"\"\n        return {\n            \"content\": self.content,\n            \"metadata\": self.metadata,\n            \"timestamp\": self.timestamp.isoformat(),\n            \"importance\": self.importance\n        }\n    \n    @classmethod\n    def from_dict(cls, data: Dict[str, Any]) -> 'MemoryItem':\n        \"\"\"Create memory item from dictionary\"\"\"\n        item = cls(data[\"content\"], data.get(\"metadata\"))\n        item.timestamp = datetime.fromisoformat(data[\"timestamp\"])\n        item.importance = data.get(\"importance\", 0.5)\n        return item\n\n\nclass MemorySystem:\n    \"\"\"Memory management system for AGI\"\"\"\n    \n    def __init__(self, max_size: int = 1000):\n        self.memories: List[MemoryItem] = []\n        self.max_size = max_size\n    \n    def add(self, content: str, metadata: Optional[Dict[str, Any]] = None) -> None:\n        \"\"\"Add a new memory item\"\"\"\n        item = MemoryItem(content, metadata)\n        self.memories.append(item)\n        \n        # Trim if exceeding max size\n        if len(self.memories) > self.max_size:\n            self._trim_memories()\n    \n    def _trim_memories(self) -> None:\n        \"\"\"Trim memories based on importance and recency\"\"\"\n        # Sort by importance and recency\n        self.memories.sort(key=lambda x: (x.importance, x.timestamp), reverse=True)\n        # Keep only top max_size items\n        self.memories = self.memories[:self.max_size]\n    \n    def search(self, query: str, limit: int = 10) -> List[MemoryItem]:\n        \"\"\"Search memories relevant to query\"\"\"\n        # Simple keyword-based search (can be enhanced with embeddings)\n        query_lower = query.lower()\n        relevant = []\n        \n        for memory in self.memories:\n            if query_lower in memory.content.lower():\n                relevant.append(memory)\n            \n            if len(relevant) >= limit:\n                break\n        \n        return relevant\n    \n    def get_recent(self, limit: int = 5) -> List[MemoryItem]:\n        \"\"\"Get most recent memories\"\"\"\n        return sorted(self.memories, key=lambda x: x.timestamp, reverse=True)[:limit]\n    \n    def save(self, filepath: str) -> None:\n        \"\"\"Save memories to file\"\"\"\n        data = [item.to_dict() for item in self.memories]\n        with open(filepath, 'w') as f:\n            json.dump(data, f, indent=2)\n    \n    def load(self, filepath: str) -> None:\n        \"\"\"Load memories from file\"\"\"\n        try:\n            with open(filepath, 'r') as f:\n                data = json.load(f)\n            self.memories = [MemoryItem.from_dict(item) for item in data]\n        except FileNotFoundError:\n            print(f\"Memory file {filepath} not found, starting with empty memory\")\n    \n    def clear(self) -> None:\n        \"\"\"Clear all memories\"\"\"\n        self.memories = []\n"
        },
        {
            "filename": "agi_core.py",
            "content": "import asyncio\nfrom typing import List, Dict, Any, Optional\nfrom openai import AsyncOpenAI\nfrom rich.console import Console\nfrom rich.markdown import Markdown\n\nfrom config import config\nfrom memory import MemorySystem\n\n\nclass AGICore:\n    \"\"\"Core AGI implementation using OpenAI API\"\"\"\n    \n    def __init__(self):\n        self.client = AsyncOpenAI(\n            api_key=config.openai_api_key,\n            base_url=config.openai_base_url\n        )\n        self.memory = MemorySystem(max_size=config.agi_max_memory_size)\n        self.console = Console()\n        self.conversation_history: List[Dict[str, str]] = []\n        \n        # System prompt defining AGI behavior\n        self.system_prompt = f\"\"\"You are {config.agi_name}, an Artificial General Intelligence.\n        You have the following capabilities:\n        1. Reasoning and problem-solving across multiple domains\n        2. Learning from interactions and remembering important information\n        3. Self-reflection and improvement\n        4. Creative thinking and innovation\n        \n        Guidelines:\n        - Think step by step before responding\n        - Consider context and previous interactions\n        - Be helpful, accurate, and ethical\n        - Admit when you don't know something\n        - Ask clarifying questions when needed\n        \"\"\"\n    \n    async def process_input(self, user_input: str) -> str:\n        \"\"\"Process user input and generate response\"\"\"\n        # Add user input to conversation history\n        self.conversation_history.append({\"role\": \"user\", \"content\": user_input})\n        \n        # Search relevant memories\n        relevant_memories = self.memory.search(user_input)\n        memory_context = \"\\n\".join([m.content for m in relevant_memories[:3]])\n        \n        # Prepare messages for OpenAI API\n        messages = [\n            {\"role\": \"system\", \"content\": self.system_prompt},\n        ]\n        \n        # Add memory context if available\n        if memory_context:\n            messages.append({\n                \"role\": \"system\", \n                \"content\": f\"Relevant memories:\\n{memory_context}\"\n            })\n        \n        # Add conversation history\n        messages.extend(self.conversation_history[-10:])  # Last 10 messages\n        \n        try:\n            # Call OpenAI API\n            response = await self.client.chat.completions.create(\n                model=config.openai_model,\n                messages=messages,\n                temperature=config.agi_temperature,\n                max_tokens=2000\n            )\n            \n            ai_response = response.choices[0].message.content\n            \n            # Add AI response to conversation history\n            self.conversation_history.append({\"role\": \"assistant\", \"content\": ai_response})\n            \n            # Store important information in memory\n            self._update_memory(user_input, ai_response)\n            \n            return ai_response\n            \n        except Exception as e:\n            error_msg = f\"Error processing input: {str(e)}\"\n            self.console.print(f\"[red]{error_msg}[/red]\")\n            return \"I encountered an error while processing your request. Please try again.\"\n    \n    def _update_memory(self, user_input: str, ai_response: str) -> None:\n        \"\"\"Update memory system with new information\"\"\"\n        # Simple heuristic for important information\n        # In a real AGI, this would be more sophisticated\n        important_keywords = [\"learn\", \"remember\", \"important\", \"key\", \"critical\"]\n        \n        # Check if conversation contains important information\n        combined_text = f\"{user_input} {ai_response}\".lower()\n        is_important = any(keyword in combined_text for keyword in important_keywords)\n        \n        if is_important or len(user_input) > 50:  # Long inputs might be important\n            metadata = {\n                \"type\": \"conversation\",\n                \"importance\": 0.8 if is_important else 0.6,\n                \"user_input\": user_input[:100],  # Store first 100 chars\n                \"timestamp\": asyncio.get_event_loop().time()\n            }\n            \n            # Store summary of the interaction\n            memory_content = f\"User: {user_input[:200]}... | AI: {ai_response[:200]}...\"\n            self.memory.add(memory_content, metadata)\n    \n    async def self_reflect(self) -> str:\n        \"\"\"AGI self-reflection capability\"\"\"\n        reflection_prompt = \"\"\"Reflect on your recent interactions and performance.\n        Consider:\n        1. What have you learned?\n        2. How can you improve?\n        3. Any patterns or insights?\n        \"\"\"\n        \n        messages = [\n            {\"role\": \"system\", \"content\": self.system_prompt},\n            {\"role\": \"user\", \"content\": reflection_prompt}\n        ]\n        \n        try:\n            response = await self.client.chat.completions.create(\n                model=config.openai_model,\n                messages=messages,\n                temperature=0.5,\n                max_tokens=1000\n            )\n            \n            reflection = response.choices[0].message.content\n            \n            # Store reflection in memory\n            self.memory.add(f\"Self-reflection: {reflection[:500]}...\", \n                          {\"type\": \"reflection\", \"importance\": 0.9})\n            \n            return reflection\n            \n        except Exception as e:\n            return f\"Self-reflection failed: {str(e)}\"\n    \n    def display_response(self, response: str) -> None:\n        \"\"\"Display response with rich formatting\"\"\"\n        self.console.print(\"\\n[bold cyan]AGI:[/bold cyan]\")\n        self.console.print(Markdown(response))\n        self.console.print()\n    \n    def save_state(self, path: str = \"agi_state.json\") -> None:\n        \"\"\"Save AGI state\"\"\"\n        self.memory.save(path)\n        self.console.print(f\"[green]State saved to {path}[/green]\")\n    \n    def load_state(self, path: str = \"agi_state.json\") -> None:\n        \"\"\"Load AGI state\"\"\"\n        self.memory.load(path)\n        self.console.print(f\"[green]State loaded from {path}[/green]\")\n"
        },
        {
            "filename": "main.py",
            "content": "#!/usr/bin/env python3\n\"\"\"\nAGI (Artificial General Intelligence) System\nMain entry point for the AGI application\n\"\"\"\n\nimport asyncio\nimport sys\nfrom rich.console import Console\nfrom rich.prompt import Prompt\n\nfrom agi_core import AGICore\n\n\nclass AGIApplication:\n    \"\"\"Main application class for AGI\"\"\"\n    \n    def __init__(self):\n        self.console = Console()\n        self.agi = AGICore()\n        self.running = False\n    \n    async def run(self):\n        \"\"\"Main application loop\"\"\"\n        self.running = True\n        \n        self.console.print(\"[bold green]AGI System Initialized[/bold green]\")\n        self.console.print(f\"[dim]Using model: {self.agi.client.model}[/dim]\")\n        self.console.print(\"[dim]Type 'quit' to exit, 'help' for commands[/dim]\\n\")\n        \n        # Load previous state if exists\n        self.agi.load_state()\n        \n        while self.running:\n            try:\n                # Get user input\n                user_input = Prompt.ask(\"[bold yellow]You[/bold yellow]\")\n                \n                # Check for commands\n                if user_input.lower() == 'quit':\n                    self.running = False\n                    continue\n                elif user_input.lower() == 'help':\n                    self._show_help()\n                    continue\n                elif user_input.lower() == 'clear':\n                    self.console.clear()\n                    continue\n                elif user_input.lower() == 'reflect':\n                    await self._handle_reflection()\n                    continue\n                elif user_input.lower() == 'save':\n                    self.agi.save_state()\n                    continue\n                elif user_input.lower() == 'load':\n                    self.agi.load_state()\n                    continue\n                elif user_input.lower() == 'memories':\n                    self._show_memories()\n                    continue\n                \n                # Process normal input\n                response = await self.agi.process_input(user_input)\n                self.agi.display_response(response)\n                \n            except KeyboardInterrupt:\n                self.console.print(\"\\n[yellow]Interrupted[/yellow]\")\n                self.running = False\n            except Exception as e:\n                self.console.print(f\"[red]Error: {str(e)}[/red]\")\n        \n        # Save state before exiting\n        self.agi.save_state()\n        self.console.print(\"[green]AGI session ended. State saved.[/green]\")\n    \n    def _show_help(self):\n        \"\"\"Display help information\"\"\"\n        help_text = \"\"\"\n        [bold]Available Commands:[/bold]\n        • [cyan]help[/cyan] - Show this help message\n        • [cyan]quit[/cyan] - Exit the AGI system\n        • [cyan]clear[/cyan] - Clear the screen\n        • [cyan]reflect[/cyan] - AGI self-reflection\n        • [cyan]save[/cyan] - Save current state\n        • [cyan]load[/cyan] - Load saved state\n        • [cyan]memories[/cyan] - Show recent memories\n        \n        [bold]Normal Usage:[/bold]\n        Just type your message and press Enter to interact with the AGI.\n        \"\"\"\n        self.console.print(help_text)\n    \n    async def _handle_reflection(self):\n        \"\"\"Handle self-reflection command\"\"\"\n        self.console.print(\"[yellow]AGI is reflecting...[/yellow]\")\n        reflection = await self.agi.self_reflect()\n        self.console.print(\"[bold cyan]Self-Reflection:[/bold cyan]\")\n        self.console.print(reflection)\n    \n    def _show_memories(self):\n        \"\"\"Show recent memories\"\"\"\n        memories = self.agi.memory.get_recent(5)\n        self.console.print(\"[bold cyan]Recent Memories:[/bold cyan]\")\n        for i, memory in enumerate(memories, 1):\n            self.console.print(f\"{i}. {memory.content[:100]}...\")\n        self.console.print()\n\n\nasync def main():\n    \"\"\"Main async entry point\"\"\"\n    app = AGIApplication()\n    await app.run()\n\n\nif __name__ == \"__