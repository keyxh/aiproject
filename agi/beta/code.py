# agi

{
    "files": [
        {
            "filename": "agi/core/agent.py",
            "content": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\nAGI Agent Core Implementation\nThis module implements the core AGI agent that uses OpenAI API for reasoning and decision making.\n\"\"\"\n\nimport asyncio\nimport logging\nfrom typing import Dict, List, Any, Optional\nfrom dataclasses import dataclass\n\nimport openai\n\nlogger = logging.getLogger(__name__)\n\n\n@dataclass\nclass Task:\n    \"\"\"Represents a task for the AGI agent to process.\"\"\"\n    id: str\n    description: str\n    context: Dict[str, Any]\n    priority: int = 0\n    status: str = \"pending\"  # pending, processing, completed, failed\n\n\nclass AGIAgent:\n    \"\"\"Core AGI agent implementation using OpenAI API.\"\"\"\n    \n    def __init__(self, openai_api_key: str):\n        \"\"\"\n        Initialize the AGI agent with OpenAI API key.\n        \n        Args:\n            openai_api_key: OpenAI API key for authentication\n        \"\"\"\n        openai.api_key = openai_api_key\n        self.tasks: List[Task] = []\n        self.memory: List[Dict[str, Any]] = []\n        self.max_memory_size = 50  # Limit memory size to prevent excessive token usage\n        \n    async def process_task(self, task: Task) -> Dict[str, Any]:\n        \"\"\"\n        Process a single task using OpenAI API.\n        \n        Args:\n            task: Task to be processed\n            \n        Returns:\n            Result of the task processing\n        \"\"\"\n        try:\n            logger.info(f\"Processing task {task.id}: {task.description}\")\n            \n            # Update task status\n            task.status = \"processing\"\n            \n            # Prepare the prompt for the AI model\n            prompt = self._construct_prompt(task)\n            \n            # Call OpenAI API\n            response = await self._call_openai_api(prompt)\n            \n            # Store in memory\n            self._add_to_memory({\n                \"task_id\": task.id,\n                \"input\": task.description,\n                \"output\": response,\n                \"timestamp\": asyncio.get_event_loop().time()\n            })\n            \n            # Update task status\n            task.status = \"completed\"\n            \n            return {\n                \"task_id\": task.id,\n                \"result\": response,\n                \"status\": \"success\"\n            }\n        except Exception as e:\n            logger.error(f\"Error processing task {task.id}: {str(e)}\")\n            task.status = \"failed\"\n            return {\n                \"task_id\": task.id,\n                \"error\": str(e),\n                \"status\": \"error\"\n            }\n    \n    def _construct_prompt(self, task: Task) -> str:\n        \"\"\"\n        Construct a prompt based on the task and available memory.\n        \n        Args:\n            task: Task to construct prompt for\n            \n        Returns:\n            Formatted prompt string\n        \"\"\"\n        # Get recent memories\n        recent_memories = self.memory[-5:] if len(self.memory) > 5 else self.memory\n        \n        memory_context = \"\\nRecent Context:\\n\"\n        for mem in recent_memories:\n            memory_context += f\"- {mem['input']} -> {mem['output'][:100]}...\\n\"\n        \n        # Construct the full prompt\n        prompt = f\"\"\"You are an advanced AGI system. Your goal is to perform complex reasoning and problem-solving tasks.\n\n{memory_context}\n\nCurrent Task: {task.description}\n\nContext: {task.context}\n\nProvide a detailed solution or response:\"\"\"\n        \n        return prompt\n    \n    async def _call_openai_api(self, prompt: str) -> str:\n        \"\"\"\n        Call OpenAI API with the given prompt.\n        \n        Args:\n            prompt: Input prompt for the API\n            \n        Returns:\n            Response from the API\n        \"\"\"\n        try:\n            # Use the newer async API call\n            response = await openai.ChatCompletion.acreate(\n                model=\"gpt-4\",\n                messages=[{\"role\": \"user\", \"content\": prompt}],\n                max_tokens=1000,\n                temperature=0.7,\n            )\n            \n            return response.choices[0].message.content.strip()\n        except Exception as e:\n            logger.error(f\"OpenAI API error: {str(e)}\")\n            raise\n    \n    def _add_to_memory(self, entry: Dict[str, Any]) -> None:\n        \"\"\"\n        Add an entry to the agent's memory.\n        \n        Args:\n            entry: Memory entry to add\n        \"\"\"\n        self.memory.append(entry)\n        \n        # Keep memory size manageable\n        if len(self.memory) > self.max_memory_size:\n            self.memory = self.memory[-self.max_memory_size:]\n    \n    async def run_task_queue(self) -> List[Dict[str, Any]]:\n        \"\"\"\n        Process all pending tasks in the queue.\n        \n        Returns:\n            List of results for each processed task\n        \"\"\"\n        results = []\n        \n        # Sort tasks by priority (higher number = higher priority)\n        sorted_tasks = sorted(self.tasks, key=lambda t: t.priority, reverse=True)\n        \n        for task in sorted_tasks:\n            if task.status == \"pending\":\n                result = await self.process_task(task)\n                results.append(result)\n        \n        return results\n    \n    def add_task(self, task: Task) -> None:\n        \"\"\"\n        Add a new task to the agent's queue.\n        \n        Args:\n            task: Task to add\n        \"\"\"\n        self.tasks.append(task)\n        logger.info(f\"Added task {task.id} to queue\")\n    \n    def get_task_by_id(self, task_id: str) -> Optional[Task]:\n        \"\"\"\n        Retrieve a task by its ID.\n        \n        Args:\n            task_id: ID of the task to retrieve\n            \n        Returns:\n            Task object if found, None otherwise\n        \"\"\"\n        for task in self.tasks:\n            if task.id == task_id:\n                return task\n        return None"
        },
        {
            "filename": "agi/core/memory_system.py",
            "content": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\nAGI Memory System\nImplements long-term and short-term memory systems for the AGI agent.\n\"\"\"\n\nimport asyncio\nimport json\nimport os\nfrom datetime import datetime\nfrom typing import Dict, List, Any, Optional\nfrom dataclasses import dataclass, asdict\n\nimport numpy as np\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n\n@dataclass\nclass MemoryEntry:\n    \"\"\"Represents a single memory entry.\"\"\"\n    id: str\n    content: str\n    timestamp: datetime\n    importance: float = 0.0\n    metadata: Dict[str, Any] = None\n    embedding: List[float] = None\n    \n    def __post_init__(self):\n        if self.metadata is None:\n            self.metadata = {}\n        if self.embedding is None:\n            self.embedding = []\n\n\nclass MemorySystem:\n    \"\"\"Memory system for the AGI agent with both short-term and long-term memory.\"\"\"\n    \n    def __init__(self, storage_path: str = \"./agi_memory.json\"):\n        \"\"\"\n        Initialize the memory system.\n        \n        Args:\n            storage_path: Path to store persistent memory\n        \"\"\"\n        self.storage_path = storage_path\n        self.short_term_memory: List[MemoryEntry] = []  # Recent memories\n        self.long_term_memory: List[MemoryEntry] = []   # Older memories\n        self.vectorizer = TfidfVectorizer(max_features=1000)\n        self.max_short_term_size = 10\n        self.load_memory()\n    \n    def store_memory(self, content: str, importance: float = 0.5, metadata: Dict[str, Any] = None) -> str:\n        \"\"\"\n        Store a new memory entry.\n        \n        Args:\n            content: Content of the memory\n            importance: Importance score (0.0 to 1.0)\n            metadata: Additional metadata for the memory\n            \n        Returns:\n            ID of the stored memory\n        \"\"\"\n        import uuid\n        \n        memory_id = str(uuid.uuid4())\n        timestamp = datetime.now()\n        \n        # Create embedding for semantic search\n        embedding = self._create_embedding(content)\n        \n        memory_entry = MemoryEntry(\n            id=memory_id,\n            content=content,\n            timestamp=timestamp,\n            importance=importance,\n            metadata=metadata or {},\n            embedding=embedding\n        )\n        \n        # Add to short-term memory\n        self.short_term_memory.append(memory_entry)\n        \n        # Move older entries to long-term memory if needed\n        self._manage_memory_size()\n        \n        # Save to persistent storage\n        self.save_memory()\n        \n        return memory_id\n    \n    def retrieve_similar_memories(self, query: str, top_k: int = 5) -> List[MemoryEntry]:\n        \"\"\"\n        Retrieve memories similar to the query.\n        \n        Args:\n            query: Query string to find similar memories\n            top_k: Number of top similar memories to return\n            \n        Returns:\n            List of similar memory entries\n        \"\"\"\n        # Combine both short-term and long-term memories\n        all_memories = self.short_term_memory + self.long_term_memory\n        \n        if not all_memories:\n            return []\n        \n        # Create embeddings for similarity calculation\n        query_embedding = self._create_embedding(query)\n        \n        # Calculate similarities\n        similarities = []\n        for memory in all_memories:\n            if memory.embedding:\n                similarity = self._cosine_similarity(query_embedding, memory.embedding)\n                similarities.append((similarity, memory))\n            \n        # Sort by similarity and return top_k\n        similarities.sort(key=lambda x: x[0], reverse=True)\n        return [mem for _, mem in similarities[:top_k]]\n    \n    def retrieve_recent_memories(self, count: int = 5) -> List[MemoryEntry]:\n        \"\"\"\n        Retrieve the most recent memories.\n        \n        Args:\n            count: Number of recent memories to retrieve\n            \n        Returns:\n            List of recent memory entries\n        \"\"\"\n        all_memories = self.short_term_memory + self.long_term_memory\n        all_memories.sort(key=lambda m: m.timestamp, reverse=True)\n        return all_memories[:count]\n    \n    def update_memory_importance(self, memory_id: str, new_importance: float) -> bool:\n        \"\"\"\n        Update the importance of a specific memory.\n        \n        Args:\n            memory_id: ID of the memory to update\n            new_importance: New importance value\n            \n        Returns:\n            True if updated successfully, False otherwise\n        \"\"\"\n        for memory in self.short_term_memory + self.long_term_memory:\n            if memory.id == memory_id:\n                memory.importance = new_importance\n                return True\n        return False\n    \n    def save_memory(self) -> None:\n        \"\"\"Save memory to persistent storage.\"\"\"\n        all_memories = self.short_term_memory + self.long_term_memory\n        \n        serialized_memories = []\n        for memory in all_memories:\n            memory_dict = asdict(memory)\n            memory_dict['timestamp'] = memory.timestamp.isoformat()\n            serialized_memories.append(memory_dict)\n        \n        with open(self.storage_path, 'w') as f:\n            json.dump(serialized_memories, f, indent=2)\n    \n    def load_memory(self) -> None:\n        \"\"\"Load memory from persistent storage.\"\"\"\n        if not os.path.exists(self.storage_path):\n            return\n        \n        try:\n            with open(self.storage_path, 'r') as f:\n                serialized_memories = json.load(f)\n            \n            for memory_data in serialized_memories:\n                timestamp = datetime.fromisoformat(memory_data['timestamp'])\n                memory_data['timestamp'] = timestamp\n                memory = MemoryEntry(**memory_data)\n                self.long_term_memory.append(memory)\n        except Exception as e:\n            print(f\"Error loading memory: {e}\")\n    \n    def _manage_memory_size(self) -> None:\n        \"\"\"Manage memory size by moving old entries to long-term memory.\"\"\"\n        while len(self.short_term_memory) > self.max_short_term_size:\n            # Move the oldest entry to long-term memory\n            oldest = min(self.short_term_memory, key=lambda m: m.timestamp)\n            self.short_term_memory.remove(oldest)\n            self.long_term_memory.append(oldest)\n    \n    def _create_embedding(self, text: str) -> List[float]:\n        \"\"\"\n        Create a simple TF-IDF embedding for the text.\n        \n        Args:\n            text: Text to create embedding for\n            \n        Returns:\n            Embedding vector as a list of floats\n        \"\"\"\n        try:\n            # Fit the vectorizer on the text (in a real implementation, you'd fit once)\n            vector = self.vectorizer.fit_transform([text]).toarray()[0]\n            return vector.tolist()\n        except:\n            # Fallback to a simple character frequency approach\n            char_freq = {}\n            for char in text.lower():\n                char_freq[char] = char_freq.get(char, 0) + 1\n            \n            # Convert to a fixed-size vector\n            vector = [char_freq.get(chr(i), 0) for i in range(97, 123)]  # a-z\n            return vector\n    \n    def _cosine_similarity(self, vec1: List[float], vec2: List[float]) -> float:\n        \"\"\"\n        Calculate cosine similarity between two vectors.\n        \n        Args:\n            vec1: First vector\n            vec2: Second vector\n            \n        Returns:\n            Cosine similarity score\n        \"\"\"\n        # Normalize vectors\n        v1 = np.array(vec1)\n        v2 = np.array(vec2)\n        \n        norm_v1 = np.linalg.norm(v1)\n        norm_v2 = np.linalg.norm(v2)\n        \n        if norm_v1 == 0 or norm_v2 == 0:\n            return 0.0\n        \n        return np.dot(v1, v2) / (norm_v1 * norm_v2)"
        },
        {
            "filename": "agi/main.py",
            "content": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\nMain entry point for the AGI system.\nThis module initializes the AGI agent and runs the main loop.\n\"\"\"\n\nimport asyncio\nimport logging\nimport os\nfrom typing import Dict, Any\n\nfrom agi.core.agent import AGIAgent, Task\nfrom agi.core.memory_system import MemorySystem\n\n\n# Configure logging\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n)\nlogger = logging.getLogger(__name__)\n\n\nclass AGISystem:\n    \"\"\"Main AGI system that orchestrates the agent and memory systems.\"\"\"\n    \n    def __init__(self, openai_api_key: str):\n        \"\"\"\n        Initialize the AGI system.\n        \n        Args:\n            openai_api_key: OpenAI API key for authentication\n        \"\"\"\n        self.agent = AGIAgent(openai_api_key)\n        self.memory_system = MemorySystem()\n        self.running = False\n    \n    async def initialize(self):\n        \"\"\"Initialize the AGI system components.\"\"\"\n        logger.info(\"Initializing AGI system...\")\n        \n        # Initialize memory system\n        self.memory_system.store_memory(\n            \"System initialized\", \n            importance=1.0, \n            metadata={\"type\": \"system_event\"}\n        )\n        \n        logger.info(\"AGI system initialized successfully\")\n    \n    async def add_task(self, description: str, context: Dict[str, Any] = None, priority: int = 0):\n        \"\"\"\n        Add a new task to the AGI system.\n        \n        Args:\n            description: Description of the task\n            context: Additional context for the task\n            priority: Priority of the task (higher number = higher priority)\n        \"\"\"\n        import uuid\n        \n        task = Task(\n            id=str(uuid.uuid4()),\n            description=description,\n            context=context or {},\n            priority=priority\n        )\n        \n        self.agent.add_task(task)\n        \n        # Store in memory\n        self.memory_system.store_memory(\n            f\"New task added: {description