# agi

{
    "files": [
        {
            "filename": "README.md",
            "content": "# AGI Project\n\nThis repository provides a minimal but extensible framework for building an Artificial General Intelligence (AGI) prototype using the OpenAI API.\n\n## Features\n- Simple **Agent** abstraction that maintains conversation memory.\n- **OpenAIClient** wrapper handling retries, streaming, and token usage logging.\n- **RESTful API** powered by Flask for external integration.\n- Configuration via environment variables or a `.env` file.\n- Type‑hints, docstrings and unit‑test scaffolding.\n\n## Quick start\n```bash\n# Clone the repo\ngit clone https://github.com/your-org/agi.git && cd agi\n\n# Install dependencies\npython -m venv .venv && source .venv/bin/activate\npip install -r requirements.txt\n\n# Set your OpenAI API key\nexport OPENAI_API_KEY=sk-your-key-here\n\n# Run the server\npython run.py\n```\n\nThe API will be available at `http://127.0.0.1:5000`. See `agi/api.py` for endpoint details.\n"
        },
        {
            "filename": "requirements.txt",
            "content": "openai>=1.0.0\nflask>=2.2.0\npython-dotenv>=1.0.0\nrequests>=2.28.0\n"
        },
        {
            "filename": "agi/__init__.py",
            "content": "# -*- coding: utf-8 -*-\n\"\"\"Top‑level package for the AGI prototype.\n\nThe package exposes the most commonly used classes so that users can simply\nimport them like:\n\n```python\nfrom agi import Agent, OpenAIClient\n```\n\"\"\"\n\nfrom .client import OpenAIClient\nfrom .agent import Agent\n\n__all__ = [\"OpenAIClient\", \"Agent\"]\n"
        },
        {
            "filename": "agi/config.py",
            "content": "# -*- coding: utf-8 -*-\n\"\"\"Configuration handling for the AGI project.\n\nConfiguration values are read from environment variables. The `python-dotenv`\npackage loads a `.env` file automatically when the module is imported.\n\"\"\"\n\nimport os\nfrom dataclasses import dataclass\nfrom pathlib import Path\nfrom typing import Literal\n\n# Load .env if present – this is a side‑effect import.\nfrom dotenv import load_dotenv\n\nload_dotenv(dotenv_path=Path(__file__).parent.parent / \".env\")\n\n\n@dataclass(frozen=True)\nclass Settings:\n    \"\"\"Immutable settings container.\n\n    Attributes:\n        openai_api_key: Your OpenAI secret key.\n        model: Default model to use (e.g., ``gpt-4o-mini``).\n        max_tokens: Upper bound for generated tokens per request.\n        temperature: Sampling temperature.\n        request_timeout: HTTP timeout in seconds.\n        log_level: Logging verbosity.\n    \"\"\"\n\n    openai_api_key: str = os.getenv(\"OPENAI_API_KEY\", \"\")\n    model: str = os.getenv(\"OPENAI_MODEL\", \"gpt-4o-mini\")\n    max_tokens: int = int(os.getenv(\"OPENAI_MAX_TOKENS\", \"1024\"))\n    temperature: float = float(os.getenv(\"OPENAI_TEMPERATURE\", \"0.7\"))\n    request_timeout: int = int(os.getenv(\"OPENAI_TIMEOUT\", \"30\"))\n    log_level: Literal[\"DEBUG\", \"INFO\", \"WARNING\", \"ERROR\", \"CRITICAL\"] = os.getenv(\n        \"LOG_LEVEL\", \"INFO\"\n    )\n\n    @property\n    def is_valid(self) -> bool:\n        \"\"\"Simple sanity check – ensure a key is provided.\"\"\"\n        return bool(self.openai_api_key)\n\n# Export a singleton for easy import.\nsettings = Settings()\n"
        },
        {
            "filename": "agi/client.py",
            "content": "# -*- coding: utf-8 -*-\n\"\"\"Thin wrapper around the OpenAI Python SDK.\n\nThe wrapper centralises:\n- API‑key handling.\n- Default request parameters.\n- Automatic retry with exponential back‑off.\n- Optional streaming support.\n- Basic usage logging.\n\"\"\"\n\nimport logging\nimport time\nfrom typing import Any, Dict, Iterable, List, Optional\n\nimport openai\nfrom openai import OpenAI\nfrom openai.types.chat import ChatCompletionMessageParam\n\nfrom .config import settings\n\nlogger = logging.getLogger(__name__)\nlogger.setLevel(settings.log_level)\n\n\nclass OpenAIClient:\n    \"\"\"Encapsulates OpenAI API interactions.\n\n    Example\n    -------\n    ```python\n    client = OpenAIClient()\n    response = client.chat(messages=[{\"role\": \"user\", \"content\": \"Hello\"}])\n    print(response)\n    ```\n    \"\"\"\n\n    def __init__(self, api_key: Optional[str] = None) -> None:\n        self.api_key = api_key or settings.openai_api_key\n        if not self.api_key:\n            raise ValueError(\"OpenAI API key must be provided via env or argument.\")\n        self.client = OpenAI(api_key=self.api_key)\n        self.default_kwargs: Dict[str, Any] = {\n            \"model\": settings.model,\n            \"max_tokens\": settings.max_tokens,\n            \"temperature\": settings.temperature,\n            \"timeout\": settings.request_timeout,\n        }\n\n    def _request_with_retry(\n        self,\n        func: Any,\n        *args: Any,\n        max_retries: int = 3,\n        backoff_factor: float = 1.5,\n        **kwargs: Any,\n    ) -> Any:\n        \"\"\"Execute *func* with exponential back‑off on transient errors.\n        \"\"\"\n        attempt = 0\n        while True:\n            try:\n                return func(*args, **kwargs)\n            except (openai.APIError, openai.RateLimitError, openai.APIConnectionError) as exc:\n                attempt += 1\n                if attempt > max_retries:\n                    logger.error(\"Max retries exceeded for OpenAI request.\")\n                    raise\n                sleep_time = backoff_factor ** (attempt - 1)\n                logger.warning(\n                    \"OpenAI request failed (attempt %d/%d). Retrying in %.1f s. Error: %s\",\n                    attempt,\n                    max_retries,\n                    sleep_time,\n                    exc,\n                )\n                time.sleep(sleep_time)\n\n    def chat(\n        self,\n        messages: List[ChatCompletionMessageParam],\n        stream: bool = False,\n    ) -> Any:\n        \"\"\"Send a chat completion request.\n\n        Parameters\n        ----------\n        messages: List of message dicts as required by the OpenAI chat API.\n        stream: If ``True`` a generator yielding incremental chunks is returned.\n        \"\"\"\n        payload = {\"messages\": messages, **self.default_kwargs}\n        logger.debug(\"Sending chat request: %s\", payload)\n        if stream:\n            return self._request_with_retry(self.client.chat.completions.create, **payload, stream=True)\n        else:\n            return self._request_with_retry(self.client.chat.completions.create, **payload)\n\n    def embed(self, input_text: str, model: str = \"text-embedding-3-large\") -> List[float]:\n        \"\"\"Generate an embedding vector for *input_text*.\n        \"\"\"\n        payload = {\"model\": model, \"input\": input_text}\n        logger.debug(\"Requesting embedding: %s\", payload)\n        resp = self._request_with_retry(self.client.embeddings.create, **payload)\n        return resp.data[0].embedding\n"
        },
        {
            "filename": "agi/agent.py",
            "content": "# -*- coding: utf-8 -*-\n\"\"\"Core *Agent* implementation.\n\nThe agent maintains a short‑term memory (conversation history) and can be\nextended with tools (e.g., web search, code execution). For the MVP we only\nprovide the chat capability backed by the :