# agi

{
    "files": [
        {
            "filename": "config.py",
            "content": "# config.py\n\"\"\"Configuration module for the AGI project.\n\nLoads environment variables and provides a central place for configuration\nconstants such as the OpenAI model names, API keys and other tunable\nparameters.\n\"\"\"\n\nimport os\nfrom pathlib import Path\nfrom dotenv import load_dotenv\n\n# Load .env file located at the project root\nBASE_DIR = Path(__file__).resolve().parent\nload_dotenv(BASE_DIR / \".env\")\n\n# ---------------------------------------------------------------------------\n# OpenAI settings\n# ---------------------------------------------------------------------------\nOPENAI_API_KEY: str = os.getenv(\"OPENAI_API_KEY\")\nif not OPENAI_API_KEY:\n    raise RuntimeError(\"OPENAI_API_KEY not set in environment or .env file\")\n\n# Model identifiers – can be overridden via environment variables if needed\nCHAT_MODEL: str = os.getenv(\"OPENAI_CHAT_MODEL\", \"gpt-4o-mini\")\nEMBEDDING_MODEL: str = os.getenv(\"OPENAI_EMBEDDING_MODEL\", \"text-embedding-3-small\")\nIMAGE_MODEL: str = os.getenv(\"OPENAI_IMAGE_MODEL\", \"dall-e-3\")\n\n# ---------------------------------------------------------------------------\n# Vector store settings (FAISS)\n# ---------------------------------------------------------------------------\nFAISS_INDEX_PATH: str = os.getenv(\"FAISS_INDEX_PATH\", str(BASE_DIR / \"faiss_index\"))\n\n# ---------------------------------------------------------------------------\n# Miscellaneous\n# ---------------------------------------------------------------------------\nMAX_TOKENS: int = int(os.getenv(\"MAX_TOKENS\", \"1024\"))\nTEMPERATURE: float = float(os.getenv(\"TEMPERATURE\", \"0.7\"))\n"
        },
        {
            "filename": "openai_client.py",
            "content": "# openai_client.py\n\"\"\"Thin wrapper around the OpenAI SDK.\n\nProvides convenient functions for chat completion, embeddings, image generation\nand any other OpenAI service required by the AGI orchestrator.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport json\nfrom typing import List, Dict, Any, Optional\n\nimport openai\nfrom config import (\n    OPENAI_API_KEY,\n    CHAT_MODEL,\n    EMBEDDING_MODEL,\n    IMAGE_MODEL,\n    MAX_TOKENS,\n    TEMPERATURE,\n)\n\n# Initialise the global OpenAI client once\nopenai.api_key = OPENAI_API_KEY\n\n\ndef chat_completion(\n    messages: List[Dict[str, str]],\n    model: str = CHAT_MODEL,\n    max_tokens: int = MAX_TOKENS,\n    temperature: float = TEMPERATURE,\n) -> str:\n    \"\"\"Call the chat completion endpoint.\n\n    Parameters\n    ----------\n    messages: List[Dict[str, str]]\n        A list of message dicts in the format required by OpenAI, e.g.\n        ``[{\"role\": \"user\", \"content\": \"Hello\"}]``.\n    model: str\n        Model identifier; defaults to the value from ``config``.\n    max_tokens: int\n        Maximum number of tokens in the response.\n    temperature: float\n        Sampling temperature.\n\n    Returns\n    -------\n    str\n        The assistant's reply.\n    \"\"\"\n    response = openai.ChatCompletion.create(\n        model=model,\n        messages=messages,\n        max_tokens=max_tokens,\n        temperature=temperature,\n    )\n    return response.choices[0].message[\"content\"].strip()\n\n\ndef embed_texts(texts: List[str], model: str = EMBEDDING_MODEL) -> List[List[float]]:\n    \"\"\"Generate embeddings for a batch of texts.\n\n    Parameters\n    ----------\n    texts: List[str]\n        The texts to embed.\n    model: str\n        Embedding model identifier.\n\n    Returns\n    -------\n    List[List[float]]\n        A list of embedding vectors (one per input text).\n    \"\"\"\n    response = openai.Embedding.create(model=model, input=texts)\n    return [item[\"embedding\"] for item in response[\"data\"]]\n\n\ndef generate_image(prompt: str, model: str = IMAGE_MODEL, n: int = 1, size: str = \"1024x1024\") -> List[str]:\n    \"\"\"Generate images from a textual prompt.\n\n    Parameters\n    ----------\n    prompt: str\n        The description of the desired image.\n    model: str\n        Image generation model identifier.\n    n: int\n        Number of images to generate.\n    size: str\n        Image resolution; e.g. ``\"1024x1024\"``.\n\n    Returns\n    -------\n    List[str]\n        URLs of the generated images.\n    \"\"\"\n    response = openai.Image.create(prompt=prompt, n=n, size=size, model=model)\n    return [item[\"url\"] for item in response[\"data\"]]\n\n\ndef moderate_content(content: str) -> bool:\n    \"\"\"Simple content moderation using OpenAI's moderation endpoint.\n\n    Returns ``True`` if the content is safe, ``False`` otherwise.\n    \"\"\"\n    response = openai.Moderation.create(input=content)\n    return not response[\"results\"][0][\"flagged\"]\n\n\n# Helper for debugging – pretty‑print the raw OpenAI response (optional)\ndef _pretty_print(response: Any) -> None:\n    print(json.dumps(response, indent=2, ensure_ascii=False))\n"
        },
        {
            "filename": "memory.py",
            "content": "# memory.py\n\"\"\"Vector‑based memory module using FAISS.\n\nThe AGI can store and retrieve contextual information across interactions.\nEmbeddings are obtained from the OpenAI embedding endpoint via\n``openai_client.embed_texts``.\n\"\"\"\n\nfrom __future__ import annotations\n\nimport os\nimport uuid\nfrom typing import List, Tuple\n\nimport faiss\nimport numpy as np\n\nfrom config import FAISS_INDEX_PATH, EMBEDDING_MODEL\nfrom openai_client import embed_texts\n\n\nclass VectorMemory:\n    \"\"\"Simple wrapper around a FAISS index for persistent semantic memory.\n\n    The index is stored on disk (``FAISS_INDEX_PATH``) and lazily loaded.\n    Each stored entry is a tuple ``(id, text)``; the id is a UUID string.\n    \"\"\"\n\n    def __init__(self, dimension: int = 1536):  # 1536 is the dim for text-embedding-3-small\n        self.dimension = dimension\n        self.index_path = FAISS_INDEX_PATH\n        self.id_to_text: dict[str, str] = {}\n        self._load_or_create_index()\n\n    # ---------------------------------------------------------------------\n    # Index handling\n    # ---------------------------------------------------------------------\n    def _load_or_create_index(self) -> None:\n        if os.path.isdir(self.index_path) and os.path.exists(os.path.join(self.index_path, \"index.faiss\")):\n            self.index = faiss.read_index(os.path.join(self.index_path, \"index.faiss\"))\n            # Load the id→text mapping\n            mapping_path = os.path.join(self.index_path, \"mapping.json\")\n            if os.path.exists(mapping_path):\n                import json\n                with open(mapping_path, \"r\", encoding=\"utf-8\") as f:\n                    self.id_to_text = json.load(f)\n        else:\n            os.makedirs(self.index_path, exist_ok=True)\n            # Use an inner product (cosine similarity) index – normalize vectors later\n            self.index = faiss.IndexFlatIP(self.dimension)\n\n    def _persist(self) -> None:\n        faiss.write_index(self.index, os.path.join(self.index_path, \"index.faiss\"))\n        import json\n