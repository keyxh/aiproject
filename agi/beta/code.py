# agi

{
    "files": [
        {
            "filename": "agi/__init__.py",
            "content": "# agi package initialization\n\n__all__ = [\"config\", \"openai_client\", \"agent\", \"main\"]\n"
        },
        {
            "filename": "agi/config.py",
            "content": "'''Configuration module for the AGI project.\n\nAll configurable parameters are defined here, making it easy to adjust\nbehaviour without touching the core logic. Sensitive data such as the\nOpenAI API key is expected to be provided via environment variables.\n''' \n\nimport os\nfrom dataclasses import dataclass\nfrom typing import List\n\n\n@dataclass(frozen=True)\nclass OpenAIConfig:\n    \"\"\"Configuration required to communicate with the OpenAI API.\"\"\"\n    api_key: str\n    base_url: str = \"https://api.openai.com/v1\"\n    model: str = \"gpt-4o-mini\"\n    temperature: float = 0.7\n    max_tokens: int = 1024\n    timeout: int = 30  # seconds\n\n    @staticmethod\n    def from_env() -> \"OpenAIConfig\":\n        \"\"\"Load configuration from environment variables.\n\n        Raises:\n            EnvironmentError: If required variables are missing.\n        \"\"\"\n        api_key = os.getenv(\"OPENAI_API_KEY\")\n        if not api_key:\n            raise EnvironmentError(\"OPENAI_API_KEY environment variable not set\")\n        # Optional overrides\n        model = os.getenv(\"OPENAI_MODEL\", \"gpt-4o-mini\")\n        temperature = float(os.getenv(\"OPENAI_TEMPERATURE\", \"0.7\"))\n        max_tokens = int(os.getenv(\"OPENAI_MAX_TOKENS\", \"1024\"))\n        timeout = int(os.getenv(\"OPENAI_TIMEOUT\", \"30\"))\n        return OpenAIConfig(\n            api_key=api_key,\n            model=model,\n            temperature=temperature,\n            max_tokens=max_tokens,\n            timeout=timeout,\n        )\n\n# Global configuration instance (loaded lazily)\n_config_instance: OpenAIConfig | None = None\n\n\ndef get_config() -> OpenAIConfig:\n    \"\"\"Return a singleton configuration object.\n\n    The configuration is loaded on first call and cached for the lifetime of the process.\n    \"\"\"\n    global _config_instance\n    if _config_instance is None:\n        _config_instance = OpenAIConfig.from_env()\n    return _config_instance\n"
        },
        {
            "filename": "agi/openai_client.py",
            "content": "'''Thin wrapper around the OpenAI HTTP API.\n\nThe wrapper isolates network concerns from the higher‑level AGI logic, making\nunit testing easier (the client can be mocked). It uses the ``httpx`` library\nbecause it provides async support and a simple synchronous interface.\n''' \n\nimport json\nfrom typing import Any, Dict, List, Optional\nimport httpx\n\nfrom .config import get_config\n\n\nclass OpenAIClient:\n    \"\"\"Synchronous client for interacting with the OpenAI chat completion endpoint.\n\n    Example\n    -------\n    >>> client = OpenAIClient()\n    >>> response = client.chat(messages=[{\"role\": \"user\", \"content\": \"Hello\"}])\n    >>> print(response[\"choices\"][0][\"message\"][\"content\"])\n    \"\"\"\n\n    def __init__(self) -> None:\n        cfg = get_config()\n        self.api_key = cfg.api_key\n        self.base_url = cfg.base_url.rstrip('/')\n        self.model = cfg.model\n        self.temperature = cfg.temperature\n        self.max_tokens = cfg.max_tokens\n        self.timeout = cfg.timeout\n        self._http_client = httpx.Client(timeout=self.timeout)\n\n    def _headers(self) -> Dict[str, str]:\n        return {\n            \"Authorization\": f\"Bearer {self.api_key}\",\n            \"Content-Type\": \"application/json\",\n        }\n\n    def chat(\n        self,\n        messages: List[Dict[str, str]],\n        functions: Optional[List[Dict[str, Any]]] = None,\n        function_call: Optional[str] = None,\n    ) -> Dict[str, Any]:\n        \"\"\"Call the chat completion endpoint.\n\n        Parameters\n        ----------\n        messages: List[Dict[str, str]]\n            Conversation history in the format required by OpenAI.\n        functions: Optional[List[Dict[str, Any]]]\n            Optional function specifications for function calling.\n        function_call: Optional[str]\n            Either \"auto\", \"none\", or the name of a specific function.\n\n        Returns\n        -------\n        Dict[str, Any]\n            Parsed JSON response from the API.\n        \"\"\"\n        payload: Dict[str, Any] = {\n            \"model\": self.model,\n            \"messages\": messages,\n            \"temperature\": self.temperature,\n            \"max_tokens\": self.max_tokens,\n        }\n        if functions is not None:\n            payload[\"functions\"] = functions\n        if function_call is not None:\n            payload[\"function_call\"] = function_call\n\n        url = f\"{self.base_url}/chat/completions\"\n        response = self._http_client.post(url, headers=self._headers(), json=payload)\n        response.raise_for_status()\n        return response.json()\n\n    def close(self) -> None:\n        \"\"\"Close underlying HTTP connections.\"\"\"\n        self._http_client.close()\n\n    # Context manager support for ``with OpenAIClient() as client:``\n    def __enter__(self) -> \"OpenAIClient\":\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb) -> None:\n        self.close()\n"
        },
        {
            "filename": "agi/agent.py",
            "content": "'''Core AGI agent implementation.\n\nThe agent maintains a conversation context, decides when to invoke\nfunctions (tools), and streams the final response back to the caller.\nFor the purpose of this prototype we implement a very simple loop:\n\n1. Receive a user query.\n2. Send the query (plus recent history) to the OpenAI model.\n3. If the model returns a function call, execute the corresponding Python\n   function (currently a placeholder that returns a static result).\n4. Append the function result to the conversation and continue until the\n   model produces a final assistant message.\n\nThe design is intentionally modular so that more sophisticated planning,\nmemory, or tool‑integration strategies can be added later.\n''' \n\nfrom __future__ import annotations\n\nimport json\nfrom typing import Any, Callable, Dict, List, Optional\n\nfrom .openai_client import OpenAIClient\n\n\n# ---------------------------------------------------------------------------\n# Simple tool registry – in a real system this would be extensible and\n# discoverable via plugins.\n# ---------------------------------------------------------------------------\n\ndef get_current_time(_: Dict[str, Any]) -> Dict[str, Any]:\n    \"\"\"Example tool that returns the current UTC time as a string.\n\n    The ``arguments`` dict is ignored for this simple demo.\n    \"\"\"\n    from datetime import datetime, timezone\n\n    return {\"time\": datetime.now(timezone.utc).isoformat()}\n\n\n_TOOL_REGISTRY: Dict[str, Callable[[Dict[str, Any]], Dict[str, Any]]] = {\n    \"get_current_time\": get_current_time,\n}\n\n\n# ---------------------------------------------------------------------------\n# Agent implementation\n# ---------------------------------------------------------------------------\n\nclass AGIAgent:\n    \"\"\"High‑level agent that orchestrates conversation with the OpenAI model.\n\n    Attributes\n    ----------\n    max_history: int\n        Maximum number of past messages to retain in the context window.\n    \"\"\"\n\n    def __init__(self, max_history: int = 10) -> None:\n        self.max_history = max_history\n        self._history: List[Dict[str, str]] = []  # List of messages in OpenAI format\n        self._client = OpenAIClient()\n\n    # -------------------------------------------------------------------\n    # Public API\n    # -------------------------------------------------------------------\n    def ask(self, user_input: str) -> str:\n        \"\"\"Process a user query and return the assistant's final reply.\n\n        Parameters\n        ----------\n        user_input: str\n            Raw text entered by the user.\n        \"\"\"\