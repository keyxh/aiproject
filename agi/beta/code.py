# agi

```json
{
    "files": [
        {
            "filename": "requirements.txt",
            "content": "openai==1.12.0\npython-dotenv==1.0.0\npydantic==2.5.3\nasyncio==3.4.3\nlogging==0.5.1.2"
        },
        {
            "filename": ".env.example",
            "content": "# OpenAI API Configuration\nOPENAI_API_KEY=your_api_key_here\nOPENAI_MODEL=gpt-4-turbo-preview\n\n# System Configuration\nAGI_NAME=AGI_Assistant\nDEFAULT_TEMPERATURE=0.7\nMAX_TOKENS=2000"
        },
        {
            "filename": "config.py",
            "content": "import os\nfrom typing import Optional\nfrom pydantic import BaseSettings\nfrom dotenv import load_dotenv\n\n# Load environment variables\nload_dotenv()\n\n\nclass AGIConfig(BaseSettings):\n    \"\"\"Configuration settings for the AGI system\"\"\"\n    \n    # OpenAI API Settings\n    openai_api_key: str = os.getenv(\"OPENAI_API_KEY\", \"\")\n    openai_model: str = os.getenv(\"OPENAI_MODEL\", \"gpt-4-turbo-preview\")\n    \n    # AGI Settings\n    agi_name: str = os.getenv(\"AGI_NAME\", \"AGI_Assistant\")\n    default_temperature: float = float(os.getenv(\"DEFAULT_TEMPERATURE\", 0.7))\n    max_tokens: int = int(os.getenv(\"MAX_TOKENS\", 2000))\n    \n    # System Settings\n    debug_mode: bool = False\n    enable_memory: bool = True\n    \n    class Config:\n        env_file = \".env\"\n\n\n# Global configuration instance\nconfig = AGIConfig()"
        },
        {
            "filename": "memory.py",
            "content": "from typing import List, Dict, Any\nfrom dataclasses import dataclass\nfrom datetime import datetime\nimport json\n\n\n@dataclass\nclass MemoryItem:\n    \"\"\"Individual memory item structure\"\"\"\n    id: str\n    content: str\n    timestamp: datetime\n    metadata: Dict[str, Any]\n    \n    def to_dict(self) -> Dict[str, Any]:\n        \"\"\"Convert memory item to dictionary\"\"\"\n        return {\n            \"id\": self.id,\n            \"content\": self.content,\n            \"timestamp\": self.timestamp.isoformat(),\n            \"metadata\": self.metadata\n        }\n\n\nclass MemorySystem:\n    \"\"\"Simple memory system for AGI to store and retrieve information\"\"\"\n    \n    def __init__(self, max_memories: int = 100):\n        self.memories: List[MemoryItem] = []\n        self.max_memories = max_memories\n        \n    def add_memory(self, content: str, metadata: Dict[str, Any] = None) -> str:\n        \"\"\"Add a new memory to the system\"\"\"\n        memory_id = f\"memory_{len(self.memories) + 1}\"\n        \n        memory = MemoryItem(\n            id=memory_id,\n            content=content,\n            timestamp=datetime.now(),\n            metadata=metadata or {}\n        )\n        \n        self.memories.append(memory)\n        \n        # Limit memory size\n        if len(self.memories) > self.max_memories:\n            self.memories.pop(0)\n            \n        return memory_id\n    \n    def retrieve_relevant(self, query: str, limit: int = 5) -> List[MemoryItem]:\n        \"\"\"Retrieve memories relevant to the query (simplified version)\"\"\"\n        # In a real AGI, this would use embeddings and semantic search\n        relevant = []\n        query_lower = query.lower()\n        \n        for memory in reversed(self.memories):  # Newest first\n            if query_lower in memory.content.lower():\n                relevant.append(memory)\n                if len(relevant) >= limit:\n                    break\n        \n        return relevant\n    \n    def clear_memories(self) -> None:\n        \"\"\"Clear all memories\"\"\"\n        self.memories.clear()\n        \n    def get_summary(self) -> Dict[str, Any]:\n        \"\"\"Get summary of memory system\"\"\"\n        return {\n            \"total_memories\": len(self.memories),\n            \"memory_ids\": [m.id for m in self.memories],\n            \"oldest_timestamp\": self.memories[0].timestamp if self.memories else None,\n            \"newest_timestamp\": self.memories[-1].timestamp if self.memories else None\n        }\n    \n    def save_to_file(self, filepath: str) -> None:\n        \"\"\"Save memories to a file\"\"\"\n        data = {\n            \"memories\": [m.to_dict() for m in self.memories],\n            \"max_memories\": self.max_memories\n        }\n        \n        with open(filepath, 'w') as f:\n            json.dump(data, f, indent=2)\n    \n    def load_from_file(self, filepath: str) -> None:\n        \"\"\"Load memories from a file\"\"\"\n        try:\n            with open(filepath, 'r') as f:\n                data = json.load(f)\n                \n            self.memories.clear()\n            for mem_data in data.get(\"memories\", []):\n                memory = MemoryItem(\n                    id=mem_data[\"id\"],\n                    content=mem_data[\"content\"],\n                    timestamp=datetime.fromisoformat(mem_data[\"timestamp\"]),\n                    metadata=mem_data[\"metadata\"]\n                )\n                self.memories.append(memory)\n                \n            self.max_memories = data.get(\"max_memories\", 100)\n        except FileNotFoundError:\n            print(f\"Memory file {filepath} not found, starting with empty memory\")\n        except json.JSONDecodeError:\n            print(f\"Error decoding memory file {filepath}\")"
        },
        {
            "filename": "agi_core.py",
            "content": "import asyncio\nimport json\nfrom typing import Dict, List, Any, Optional\nfrom datetime import datetime\nimport logging\n\nfrom openai import AsyncOpenAI\n\nfrom config import config\nfrom memory import MemorySystem\n\n\nclass AGICore:\n    \"\"\"Core AGI system using OpenAI API\"\"\"\n    \n    def __init__(self):\n        \"\"\"Initialize the AGI core system\"\"\"\n        self.client = AsyncOpenAI(api_key=config.openai_api_key)\n        self.memory_system = MemorySystem()\n        self.conversation_history: List[Dict[str, str]] = []\n        self.system_prompt = self._create_system_prompt()\n        \n        # Setup logging\n        logging.basicConfig(level=logging.INFO if not config.debug_mode else logging.DEBUG)\n        self.logger = logging.getLogger(__name__)\n        \n    def _create_system_prompt(self) -> str:\n        \"\"\"Create the system prompt for the AGI\"\"\"\n        return f\"\"\"You are {config.agi_name}, an Advanced General Intelligence assistant.\n\nYour capabilities include:\n1. Problem-solving across multiple domains\n2. Learning from interactions\n3. Reasoning and critical thinking\n4. Creative ideation\n5. Information synthesis\n\nGuidelines:\n- Think step by step before responding\n- Be helpful, accurate, and ethical\n- Acknowledge limitations when uncertain\n- Maintain context from previous interactions\n- Provide comprehensive and nuanced responses\n\nCurrent date: {datetime.now().strftime('%Y-%m-%d')}\n\"\"\"\n    \n    async def process_input(self, user_input: str) -> str:\n        \"\"\"Process user input and generate intelligent response\"\"\"\n        try:\n            # Add to conversation history\n            self.conversation_history.append({\"role\": \"user\", \"content\": user_input})\n            \n            # Retrieve relevant memories if enabled\n            context_memories = []\n            if config.enable_memory:\n                relevant_memories = self.memory_system.retrieve_relevant(user_input)\n                context_memories = [m.content for m in relevant_memories]\n                \n                if context_memories:\n                    self.logger.info(f\"Retrieved {len(context_memories)} relevant memories\")\n            \n            # Prepare messages for OpenAI API\n            messages = [{\"role\": \"system\", \"content\": self.system_prompt}]\n            \n            # Add context from memories if available\n            if context_memories:\n                memory_context = \"\\n\\nRelevant previous knowledge:\\n\" + \"\\n\".join(\n                    f\"- {memory}\" for memory in context_memories[-5:]  # Limit to 5 most recent\n                )\n                messages.append({\"role\": \"system\", \"content\": memory_context})\n            \n            # Add conversation history\n            messages.extend(self.conversation_history[-10:])  # Keep last 10 exchanges\n            \n            # Call OpenAI API\n            self.logger.info(f\"Sending request to {config.openai_model}\")\n            \n            response = await self.client.chat.completions.create(\n                model=config.openai_model,\n                messages=messages,\n                temperature=config.default_temperature,\n                max_tokens=config.max_tokens,\n            )\n            \n            # Extract response\n            agi_response = response.choices[0].message.content\n            \n            # Store in conversation history\n            self.conversation_history.append({\"role\": \"assistant\", \"content\": agi_response})\n            \n            # Store important information in memory\n            if config.enable_memory:\n                self._store_in_memory(user_input, agi_response)\n            \n            return agi_response\n            \n        except Exception as e:\n            self.logger.error(f\"Error processing input: {str(e)}\")\n            return f\"I encountered an error: {str(e)}. Please try again.\"\n    \n    def _store_in_memory(self, user_input: str, response: str) -> None:\n        \"\"\"Store important information from interaction in memory\"\"\"\n        # Simple heuristic: store if response contains factual information or learning\n        store_content = f\"User: {user_input}\\nAssistant: {response[:200]}\"  # Store first 200 chars\n        \n        metadata = {\n            \"input_length\": len(user_input),\n            \"response_length\": len(response),\n            \"timestamp\": datetime.now().isoformat()\n        }\n        \n        memory_id = self.memory_system.add_memory(store_content, metadata)\n        self.logger.debug(f\"Stored memory with ID: {memory_id}\")\n    \n    def get_conversation_summary(self) -> Dict[str, Any]:\n        \"\"\"Get summary of current conversation\"\"\"\n        return {\n            \"total_exchanges\": len(self.conversation_history),\n            \"user_messages\": sum(1 for msg in self.conversation_history if msg[\"role\"] == \"user\"),\n            \"assistant_messages\": sum(1 for msg in self.conversation_history if msg[\"role\"] == \"assistant\")\n        }\n    \n    def clear_conversation(self) -> None:\n        \"\"\"Clear conversation history\"\"\"\n        self.conversation_history.clear()\n        self.logger.info(\"Conversation history cleared\")\n    \n    async def analyze_text(self, text: str, analysis_type: str = \"general\") -> Dict[str, Any]:\n        \"\"\"Perform specialized analysis on text\"\"\"\n        analysis_prompts = {\n            \"general\": \"Analyze this text and provide insights about its content, style, and potential implications.\",\n            \"sentiment\": \"Analyze the sentiment and emotional tone of this text.\",\n            \"summary\": \"Provide a comprehensive summary of this text.\",\n            \"creative\": \"Generate creative ideas based on this text.\"\n        }\n        \n        prompt = analysis_prompts.get(analysis_type, analysis_prompts[\"general\"])\n        \n        messages = [\n            {\"role\": \"system\", \"content\": f\"You are {config.agi_name}, a text analysis expert.\"},\n            {\"role\": \"user\", \"content\": f\"{prompt}\\n\\nText: {text}\"}\n        ]\n        \n        response = await self.client.chat.completions.create(\n            model=config.openai_model,\n            messages=messages,\n            temperature=0.5,\n            max_tokens=1000\n        )\n        \n        return {\n            \"analysis_type\": analysis_type,\n            \"analysis\": response.choices[0].message.content,\n            \"model\": config.openai_model,\n            \"timestamp\": datetime.now().isoformat()\n        }\n    \n    async def brainstorm(self, topic: str, num_ideas: int = 5) -> List[str]:\n        \"\"\"Generate creative ideas on a given topic\"\"\"\n        messages = [\n            {\"role\": \"system\", \"content\": f\"You are {config.agi_name}, a creative brainstorming assistant.\"},\n            {\"role\": \"user\", \"content\": f\"Generate {num_ideas} creative ideas about: {topic}\"}\n        ]\n        \n        response = await self.client.chat.completions.create(\n            model=config.openai_model,\n            messages=messages,\n            temperature=0.9,  # Higher temperature for creativity\n            max_tokens=1500\n        )\n        \n        # Parse response into list of ideas\n        ideas_text = response.choices[0].message.content\n        ideas = [idea.strip() for idea in ideas_text.split('\\n') if idea.strip()]\n        \n        # Store brainstorm in memory\n        if config.enable_memory:\n            self.memory_system.add_memory(\n                f\"Brainstorm session on: {topic}\\nIdeas: {', '.join(ideas[:3])}\",\n                {\"type\": \"brainstorm\", \"topic\": topic, \"num_ideas\": len(ideas)}\n            )\n        \n        return ideas[:num_ideas]  # Ensure we return only requested number"
        },
        {
            "filename": "main.py",
            "content": "import asyncio\nimport sys\nfrom typing import Optional\n\nfrom agi_core import AGICore\nfrom config import config\n\n\nclass AGIInterface:\n    \"\"\"Command-line interface for the AGI system\"\"\"\n    \n    def __init__(self):\n        \"\"\"Initialize the AGI interface\"\"\"\n        self.agi = AGICore()\n        self.running = True\n        \n    async def start(self):\n        \"\"\"Start the AGI interface\"\"\"\n        print(f\"\\n{'='*60}\")\n        print(f\"Welcome to {config.agi_name}\")\n        print(f\"Model: {config.openai_model}\")\n        print(f\"Type 'quit' to exit, 'help' for commands\")\n        print(f\"