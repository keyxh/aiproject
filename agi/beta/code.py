# agi

{
    "files": [
        {
            "filename": "agi_core.py",
            "content": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\nAGI Core Module\n\nThis module implements the core functionality of a General Artificial Intelligence (AGI)\nsystem using OpenAI's API as the underlying language model. The system is designed to\nsupport self-improvement, long-term memory, goal-directed reasoning, and adaptive learning.\n\nAuthor: Intermediate Engineer\nProject: AGI\n\"\"\"\n\nimport asyncio\nimport json\nimport logging\nimport time\nfrom typing import Dict, List, Optional, Any, Callable\n\nimport openai\nfrom openai import OpenAI\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n\nclass AGIEngine:\n    \"\"\"\n    A General Artificial Intelligence Engine powered by OpenAI API.\n\n    This class manages the core intelligence loop: perception → reasoning → action → learning.\n    It supports dynamic goal setting, memory storage, and iterative self-improvement.\n    \"\"\"\n\n    def __init__(\n        self,\n        api_key: str,\n        model_name: str = \"gpt-4-turbo\",\n        max_tokens: int = 4096,\n        temperature: float = 0.7,\n        top_p: float = 1.0,\n        frequency_penalty: float = 0.0,\n        presence_penalty: float = 0.0\n    ):\n        \"\"\"\n        Initialize the AGI engine with OpenAI API configuration.\n\n        Args:\n            api_key (str): OpenAI API key\n            model_name (str): Name of the OpenAI model to use\n            max_tokens (int): Maximum tokens in response\n            temperature (float): Sampling temperature for creativity\n            top_p (float): Nucleus sampling probability\n            frequency_penalty (float): Penalty for token repetition\n            presence_penalty (float): Penalty for introducing new topics\n        \"\"\"\n        self.api_key = api_key\n        self.model_name = model_name\n        self.max_tokens = max_tokens\n        self.temperature = temperature\n        self.top_p = top_p\n        self.frequency_penalty = frequency_penalty\n        self.presence_penalty = presence_penalty\n\n        # Initialize OpenAI client\n        self.client = OpenAI(api_key=api_key)\n\n        # Internal state\n        self.goals: List[str] = []\n        self.memory: List[Dict[str, Any]] = []\n        self.context_history: List[Dict[str, Any]] = []\n        self.last_response: Optional[str] = None\n        self.conversation_id: str = f\"agi_{int(time.time())}\"\n\n        logger.info(f\"AGI Engine initialized with model {model_name}\")\n\n    def add_goal(self, goal: str) -> None:\n        \"\"\"\n        Add a new goal to the AGI's objective list.\n\n        Args:\n            goal (str): Description of the goal\n        \"\"\"\n        if goal not in self.goals:\n            self.goals.append(goal)\n            logger.info(f\"Added new goal: {goal}\")\n\n    def get_goals(self) -> List[str]:\n        \"\"\"\n        Return current list of goals.\n\n        Returns:\n            List[str]: Current goals\n        \"\"\"\n        return self.goals.copy()\n\n    def store_memory(self, content: str, metadata: Optional[Dict[str, Any]] = None) -> None:\n        \"\"\"\n        Store a piece of information in long-term memory.\n\n        Args:\n            content (str): Content to store\n            metadata (dict, optional): Additional context about the memory\n        \"\"\"\n        memory_entry = {\n            \"timestamp\": time.time(),\n            \"content\": content,\n            \"metadata\": metadata or {}\n        }\n        self.memory.append(memory_entry)\n        logger.debug(f\"Stored memory: {content[:50]}...\")\n\n    def retrieve_relevant_memory(self, query: str, limit: int = 5) -> List[str]:\n        \"\"\"\n        Retrieve up to `limit` most relevant memories based on similarity to query.\n\n        Args:\n            query (str): Query to find relevant memories\n            limit (int): Maximum number of memories to return\n\n        Returns:\n            List[str]: Relevant memory contents\n        \"\"\"\n        if not self.memory:\n            return []\n\n        # Simple heuristic: match keywords\n        results = []\n        for entry in self.memory:\n            if any(keyword.lower() in entry[\"content\"].lower() for keyword in query.split()):\n                results.append(entry[\"content\"])\n                if len(results) >= limit:\n                    break\n\n        return results\n\n    async def generate_response(self, prompt: str, context: Optional[str] = None) -> str:\n        \"\"\"\n        Generate a response using OpenAI API with context-aware prompting.\n\n        Args:\n            prompt (str): Main request or question\n            context (str, optional): Additional context (e.g., from memory)\n\n        Returns:\n            str: Generated response\n        \"\"\"\n        try:\n            # Build system message with current goals and memory context\n            system_prompt = f\"You are an advanced general AI agent. Your primary goals are: {', '.join(self.goals)}.\"\n            \n            # Include relevant memory if available\n            if context:\n                system_prompt += f\" Previous knowledge: {context}\"\n            \n            # Prepare messages for OpenAI\n            messages = [\n                {\"role\": \"system\", \"content\": system_prompt},\n                {\"role\": \"user\", \"content\": prompt}\n            ]\n\n            # Call OpenAI API\n            response = await asyncio.to_thread(\n                self.client.chat.completions.create,\n                model=self.model_name,\n                messages=messages,\n                max_tokens=self.max_tokens,\n                temperature=self.temperature,\n                top_p=self.top_p,\n                frequency_penalty=self.frequency_penalty,\n                presence_penalty=self.presence_penalty\n            )\n\n            # Extract and store response\n            generated_text = response.choices[0].message.content\n            self.last_response = generated_text\n\n            # Update conversation history\n            self.context_history.append({\n                \"prompt\": prompt,\n                \"response\": generated_text,\n                \"timestamp\": time.time()\n            })\n\n            logger.info(\"Response generated successfully\")\n            return generated_text\n\n        except Exception as e:\n            logger.error(f\"Error generating response: {e}\")\n            raise\n\n    async def self_improve(self) -> None:\n        \"\"\"\n        Self-improve the AGI by analyzing past performance and refining strategies.\n        \"\"\"\n        improvement_prompt = (\n            \"Analyze your recent interactions and identify ways to improve. Consider: \"\n            \"accuracy, clarity, efficiency, and alignment with goals. Suggest specific changes.\"\n        )\n\n        try:\n            improvement_result = await self.generate_response(improvement_prompt)\n            self.store_memory(f\"Self-improvement suggestion: {improvement_result}\", {\"type\": \"self_reflection\"})\n            logger.info(\"Self-improvement analysis completed\")\n        except Exception as e:\n            logger.warning(f\"Failed to perform self-improvement: {e}\")\n\n    async def execute_task(self, task_description: str) -> Dict[str, Any]:\n        \"\"\"\n        Execute a task through the full AGI loop: goal setting → reasoning → action → reflection.\n\n        Args:\n            task_description (str): Description of the task to perform\n\n        Returns:\n            Dict[str, Any]: Result including output and metadata\n        \"\"\"\n        logger.info(f\"Executing task: {task_description}\")\n\n        # Step 1: Set or verify goal\n        if not self.goals:\n            self.add_goal(task_description)\n\n        # Step 2: Retrieve relevant memory\n        relevant_memories = self.retrieve_relevant_memory(task_description)\n        context = \" | \".join(relevant_memories) if relevant_memories else None\n\n        # Step 3: Generate response\n        response = await self.generate_response(task_description, context)\n\n        # Step 4: Store result in memory\n        self.store_memory(response, {\"task\": task_description, \"status\": \"completed\"})\n\n        # Step 5: Self-improve\n        await self.self_improve()\n\n        # Return result\n        return {\n            \"task\": task_description,\n            \"result\": response,\n            \"success\": True,\n            \"memory_stored\": len(self.memory),\n            \"timestamp\": time.time()\n        }\n\n    def save_state(self, filepath: str) -> None:\n        \"\"\"\n        Save current state to a JSON file.\n\n        Args:\n            filepath (str): Path to save the state\n        \"\"\"\n        state = {\n            \"goals\": self.goals,\n            \"memory\": self.memory,\n            \"context_history\": self.context_history,\n            \"last_response\": self.last_response,\n            \"conversation_id\": self.conversation_id\n        }\n\n        with open(filepath, 'w') as f:\n            json.dump(state, f, indent=2)\n\n        logger.info(f\"State saved to {filepath}\")\n\n    def load_state(self, filepath: str) -> None:\n        \"\"\"\n        Load state from a JSON file.\n\n        Args:\n            filepath (str): Path to load state from\n        \"\"\"\n        try:\n            with open(filepath, 'r') as f:\n                state = json.load(f)\n\n            self.goals = state.get(\"goals\", [])\n            self.memory = state.get(\"memory\", [])\n            self.context_history = state.get(\"context_history\", [])\n            self.last_response = state.get(\"last_response\")\n            self.conversation_id = state.get(\"conversation_id\", f\"agi_{int(time.time())}\")\n\n            logger.info(f\"State loaded from {filepath}\")\n        except Exception as e:\n            logger.error(f\"Failed to load state from {filepath}: {e}\")\n\n    def reset(self) -> None:\n        \"\"\"\n        Reset the AGI engine to initial state.\n        \"\"\"\n        self.goals = []\n        self.memory = []\n        self.context_history = []\n        self.last_response = None\n        self.conversation_id = f\"agi_{int(time.time())}\"\n        logger.info(\"AGI engine reset\")\n\n\n# Example usage and testing\nif __name__ == \"__main__\":\n    # Example: Run a simple test\n    async def main():\n        # Replace with your actual OpenAI API key\n        api_key = \"YOUR_OPENAI_API_KEY\"\n\n        agi = AGIEngine(api_key=api_key, model_name=\"gpt-4-turbo\")\n\n        # Add a goal\n        agi.add_goal(\"Write a poem about the future\")\n\n        # Execute a task\n        result = await agi.execute_task(\"Write a poem about the future\")\n        print(f\"Result: {result['result']}\")\n\n        # Save state\n        agi.save_state(\"agi_state.json\")\n\n        # Clean up\n        agi.reset()\n\n    # Run the example\n    asyncio.run(main())"
        },
        {
            "filename": "config.py",
            "content": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\nConfiguration module for AGI project.\n\nContains default configurations for the AGI system, including API settings,\nmodel parameters, and system behavior.\n\nAuthor: Intermediate Engineer\nProject: AGI\n\"\"\"\n\nimport os\nfrom dataclasses import dataclass\nfrom typing import Optional\n\n\n@dataclass\nclass AGIConfig:\n    \"\"\"\n    Configuration class for the AGI system.\n    \"\"\"\n\n    # OpenAI API Settings\n    OPENAI_API_KEY: str = os.getenv(\"OPENAI_API_KEY\", \"\")\n    MODEL_NAME: str = \"gpt-4-turbo\"\n    MAX_TOKENS: int = 4096\n    TEMPERATURE: float = 0.7\n    TOP_P: float = 1.0\n    FREQUENCY_PENALTY: float = 0.0\n    PRESENCE_PENALTY: float = 0.0\n\n    # Memory Settings\n    MEMORY_RETENTION_DAYS: int = 365\n    MAX_MEMORY_SIZE: int = 1000\n\n    # Self-Improvement Settings\n    SELF_IMPROVE_INTERVAL: int = 3  # Every 3 tasks\n    SELF_IMPROVE_THRESHOLD: int = 5  # After 5 tasks\n\n    # Logging Settings\n    LOG_LEVEL: str = \"INFO\"\n    LOG_FILE: Optional[str] = \"agi.log\"\n\n    # State Persistence\n    STATE_FILE: str = \"agi_state.json\"\n    AUTO_SAVE_INTERVAL: int = 60  # seconds\n\n    # System Behavior\n    ENABLE_SELF_REFLECTION: bool = True\n    ENABLE_LONG_TERM_PLANNING: bool = True\n\n    def validate(self) -> bool:\n        \"\"\"\n        Validate configuration values.\n\n        Returns:\n            bool: True if all required fields are valid\n        \"\"\"\n        if not self.OPENAI_API_KEY:\n            raise ValueError(\"OpenAI API key is required\")\n\n        if self.TEMPERATURE < 0 or self.TEMPERATURE > 1:\n            raise ValueError(\"Temperature must be between 0 and 1\")\n\n        if self.TOP_P < 0 or self.TOP_P > 1:\n            raise ValueError(\"Top P must be between 0 and 1\")\n\n        if self.MAX_TOKENS <= 0:\n            raise ValueError(\"Max tokens must be positive\")\n\n        return True\n\n    def to_dict(self) -> dict:\n        \"\"\"\n        Convert configuration to dictionary.\n\n        Returns:\n            dict: Configuration as dictionary\n        \"\"\"\n        return {k: v for k, v in self.__dict__.items() if not k.startswith('_')}\n\n\n# Default configuration instance\nCONFIG = AGIConfig()\n\n# Override with environment variables if provided\nif os.getenv(\"AGI_MODEL_NAME\"):\n    CONFIG.MODEL_NAME = os.getenv(\"AGI_MODEL_NAME\")\nif os.getenv(\"AGI_TEMPERATURE\"):\n    CONFIG.TEMPERATURE = float(os.getenv(\"AGI_TEMPERATURE\"))\nif os.getenv(\"AGI_MAX_TOKENS\"):\n    CONFIG.MAX_TOKENS = int(os.getenv(\"AGI_MAX_TOKENS\"))\nif os.getenv(\"AGI_LOG_LEVEL\"):\n    CONFIG.LOG_LEVEL = os.getenv(\"AGI_LOG_LEVEL\").upper()\n\n# Validate config\ntry:\n    CONFIG.validate()\nexcept ValueError as e:\n    raise RuntimeError(f\"Invalid configuration: {e}\")\n\n# Export for easy access\n__all__ = [\"CONFIG\", \"AGIConfig\"]"
        },
        {
            "filename": "main.py",
            "content": "#!/usr/bin/env python3\n# -*- coding: utf-8 -*-\n\"\"\"\nMain entry point for the AGI system.\n\nThis script initializes the AGI engine, loads configuration, and provides\na command-line interface for interacting with the AGI.\n\nAuthor: Intermediate Engineer\nProject: AGI\n\"\"\"\n\nimport argparse\nimport asyncio\nimport json\nimport logging\nimport sys\nfrom pathlib import Path\n\nfrom agi_core import AGIEngine\nfrom config import CONFIG\n\n# Configure logging\nlogging.basicConfig(\n    level=getattr(logging, CONFIG.LOG_LEVEL, logging.INFO),\n    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n)\nlogger = logging.getLogger(__name__)\n\n\nasync def run_interactive_mode(agi: AGIEngine):\n    \"\"\"\n    Run the AGI in interactive mode.\n\n    Args:\n        agi (AGIEngine): Initialized AGI engine\n    \"\"\"\n    print(\"\\n=== Welcome to the AGI System ===\")\n    print(\"Type 'quit' to exit, 'help' for commands, 'goals' to see current goals\")\n\n    while True:\n        try:\n            user_input = input(\"\\n> \").strip()\n            if not user_input:\n                continue\n\n            if user_input.lower() in ['quit', 'exit']:\n                print(\"Goodbye!\")\n                break\n\n            if user_input.lower() == 'help':\n                print(\"Commands:\")\n                print(\"  goals     - Show current goals\")\n                print(\"  memory    - Show recent memory\")\n                print(\"  save      - Save current state\")\n                print(\"  load      - Load state from file\")\n                print(\"  reset     - Reset the AGI\")\n                print(\"  help      - Show this help\")\n                print(\"  <anything>- Submit a task to the AGI\")\n                continue\n\n            if user_input.lower() == 'goals':\n                goals = agi.get_goals()\n                if goals:\n                    print(\"Current goals:\")\n                    for i, goal in enumerate(goals, 1):\n                        print(f\"  {i}. {goal}\")\n                else:\n                    print(\"No goals set yet.\")\n                continue\n\n            if user_input.lower() == 'memory':\n                if agi.memory:\n                    print(\"Recent memory entries:\")\n                    for entry in agi.memory[-5:]: