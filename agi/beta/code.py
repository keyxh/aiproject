# agi

{
    "files": [
        {
            "filename": "agi/architecture/design.md",
            "content": "# AGI ç³»ç»Ÿæ¶æ„è®¾è®¡æ–‡æ¡£\n\n## 1. æ¦‚è¿°\næœ¬é¡¹ç›®æ—¨åœ¨æ„å»ºä¸€ä¸ªå…·å¤‡é€šç”¨è®¤çŸ¥èƒ½åŠ›çš„AGIï¼ˆArtificial General Intelligenceï¼‰ç³»ç»Ÿï¼Œä»¥æ¨¡å—åŒ–ã€å¯æ‰©å±•ã€å®‰å…¨å¯æ§ä¸ºè®¾è®¡åŸåˆ™ï¼Œé€šè¿‡ç»„åˆå¤šæ¨¡æ€æ„ŸçŸ¥ã€è®°å¿†ã€æ¨ç†ã€è§„åˆ’ä¸æ‰§è¡Œèƒ½åŠ›ï¼Œå®ç°è·¨ä»»åŠ¡ã€è·¨é¢†åŸŸçš„è‡ªä¸»æ™ºèƒ½ã€‚\n\n> **æ ¸å¿ƒç†å¿µ**ï¼šAGI â‰  æ›´å¤§çš„LLMã€‚AGI æ˜¯ä¸€ä¸ª**è®¤çŸ¥æ“ä½œç³»ç»Ÿ**ï¼ˆCognitive OSï¼‰ï¼ŒLLM æ˜¯å…¶æ ¸å¿ƒæ¨ç†å¼•æ“ä¹‹ä¸€ã€‚\n\n---\n\n## 2. æ¶æ„æ€»è§ˆï¼ˆåˆ†å±‚æ¨¡å‹ï¼‰\n\n```\n+---------------------------+\n|      ğŸ§  Executive Layer   | â† å…¨å±€ç›®æ ‡ç®¡ç†ã€å…ƒè®¤çŸ¥ã€è‡ªæˆ‘åæ€\n+---------------------------+\n|    ğŸ§© Reasoning Orchestrator | â† åŠ¨æ€è°ƒç”¨å­æ¨¡å—ã€è§„åˆ’è·¯å¾„ã€éªŒè¯ç»“æœ\n+---------------------------+\n| ğŸ“¦ Core Cognitive Modules |\n| â€¢ Memory (Short/Long-term) |\n| â€¢ Perception (Vision/Audio/NLP) |\n| â€¢ Planning & Simulation    |\n| â€¢ Tool Execution Engine    |\n| â€¢ Self-Improvement Loop    |\n+---------------------------+\n|     ğŸ¤– LLM Backend (OpenAI API) |\n| â€¢ gpt-4o / o1-preview / future models |\n| â€¢ Function Calling + JSON Schema |\n| â€¢ Multi-turn, stateful sessions |\n+---------------------------+\n|     ğŸŒ External Interfaces  |\n| â€¢ Web/API Tools (Search, Code, DB) |\n| â€¢ Human-in-the-loop (HITL)   |\n| â€¢ Environment Simulators     |\n+---------------------------+\n```\n\n---\n\n## 3. æ ¸å¿ƒç»„ä»¶è®¾è®¡\n\n### 3.1 è®¤çŸ¥ä¸»æ§å™¨ï¼ˆExecutive Controllerï¼‰\n- è´Ÿè´£æ¥æ”¶é«˜å±‚ç›®æ ‡ï¼ˆå¦‚ï¼šâ€œä¸ºç”¨æˆ·å‡†å¤‡ä¸€ä»½å¯æŒç»­å‘å±•æŠ¥å‘Šâ€ï¼‰\n- åˆ†è§£ä¸ºå­ä»»åŠ¡æµï¼ˆæ£€ç´¢â†’åˆ†æâ†’æ’°å†™â†’æ ¡éªŒâ†’å‘ˆç°ï¼‰\n- ç›‘æ§å„æ¨¡å—çŠ¶æ€ï¼ŒåŠ¨æ€é‡è§„åˆ’\n- å®ç°**è‡ªæˆ‘è¯„ä¼°**ï¼šæ¯å®Œæˆä¸€é˜¶æ®µï¼Œç”Ÿæˆåæ€æ‘˜è¦å¹¶æ›´æ–°å†…éƒ¨ä¿¡å¿µ\n\n### 3.2 è®°å¿†ç³»ç»Ÿï¼ˆMemory Systemï¼‰\n- **çŸ­æœŸè®°å¿†**ï¼šåŸºäºå‘é‡æ•°æ®åº“ï¼ˆå¦‚ Chroma / Qdrantï¼‰ï¼Œæ”¯æŒè¯­ä¹‰æ£€ç´¢ä¸ä¸Šä¸‹æ–‡å‹ç¼©\n- **é•¿æœŸè®°å¿†**ï¼šç»“æ„åŒ–çŸ¥è¯†å›¾è°±ï¼ˆNeo4jï¼‰ï¼Œå­˜å‚¨å®ä½“å…³ç³»ã€å› æœé“¾ã€ç»éªŒæ•™è®­\n- **å·¥ä½œè®°å¿†**ï¼šä¸´æ—¶ä¼šè¯çŠ¶æ€ï¼ˆRedisï¼‰ï¼Œæ”¯æŒè·¨è½®æ¬¡æ¨ç†é“¾è¿½è¸ª\n\n### 3.3 æ¨ç†ç¼–æ’å™¨ï¼ˆReasoning Orchestratorï¼‰\n- æ”¯æŒå¤šç§æ¨ç†èŒƒå¼ï¼š\n  - **Chain-of-Thought**ï¼ˆCoTï¼‰\n  - **Tree-of-Thought**ï¼ˆToTï¼‰\n  - **Graph-of-Thought**ï¼ˆGoTï¼‰\n  - **Algorithmic Reasoning**ï¼ˆè°ƒç”¨Pythonæ²™ç®±æ‰§è¡Œä»£ç ï¼‰\n- ä½¿ç”¨ `LangGraph` æˆ–è‡ªç ” DAG æ‰§è¡Œå¼•æ“è°ƒåº¦\n\n### 3.4 å·¥å…·æ‰§è¡Œå¼•æ“ï¼ˆTool Executorï¼‰\n- ç»Ÿä¸€å·¥å…·æ³¨å†Œä¸è°ƒç”¨æ¥å£\n- æ”¯æŒï¼š\n  - OpenAI Function Callingï¼ˆæ ‡å‡†JSON Schemaï¼‰\n  - è‡ªå®šä¹‰å·¥å…·ï¼ˆå¦‚ `search_web`, `execute_python`, `query_db`ï¼‰\n  - å®‰å…¨æ²™ç®±ï¼šæ‰€æœ‰ä»£ç æ‰§è¡Œåœ¨éš”ç¦»å®¹å™¨ä¸­ï¼ˆDocker + seccompï¼‰\n\n### 3.5 è‡ªæˆ‘æ”¹è¿›å¾ªç¯ï¼ˆSelf-Improvement Loopï¼‰\n- æ¯æ¬¡ä»»åŠ¡å®Œæˆåï¼Œè§¦å‘ï¼š\n  1. **ç»“æœå›æº¯**ï¼šå¯¹æ¯”é¢„æœŸ vs å®é™…è¾“å‡º\n  2. **é”™è¯¯å½’å› **ï¼šå®šä½å¤±è´¥ç¯èŠ‚ï¼ˆè®°å¿†ç¼ºå¤±ï¼Ÿæ¨ç†åå·®ï¼Ÿå·¥å…·è¯¯ç”¨ï¼Ÿï¼‰\n  3. **ç­–ç•¥æ›´æ–°**ï¼šç”Ÿæˆæ–°æç¤ºæ¨¡æ¿ / å¾®è°ƒè®°å¿†ç´¢å¼• / æ³¨å†Œæ–°å·¥å…·\n  4. **A/Bæµ‹è¯•**ï¼šå¯¹å…³é”®è·¯å¾„è¿›è¡Œå¹¶è¡Œæ¨ç†å¯¹æ¯”\n- å¯é€‰ï¼šç»“åˆäººç±»åé¦ˆï¼ˆRLHF+PPOï¼‰å¾®è°ƒå±€éƒ¨ç­–ç•¥\n\n---\n\n## 4. OpenAI API é›†æˆè§„èŒƒ\n\n### 4.1 è¯·æ±‚å°è£…å±‚\n```python\n# agi/core/llm_client.py\nfrom openai import AsyncOpenAI\nfrom pydantic import BaseModel\nfrom typing import List, Dict, Optional\n\nclass LLMClient:\n    def __init__(self, api_key: str, base_url: str = \"https://api.openai.com/v1\"):\n        self.client = AsyncOpenAI(api_key=api_key, base_url=base_url)\n        \n    async def chat_completion(\n        self,\n        messages: List[Dict],\n        tools: Optional[List[Dict]] = None,\n        tool_choice: Optional[str] = \"auto\",\n        response_format: Optional[BaseModel] = None,\n        temperature: float = 0.3,\n        max_tokens: int = 2048,\n    ) -> Dict:\n        # âœ… å¼ºåˆ¶å¯ç”¨ JSON Schema å“åº”æ ¼å¼ï¼ˆè‹¥æŒ‡å®šï¼‰\n        # âœ… è‡ªåŠ¨æ³¨å…¥ç³»ç»Ÿè§’è‰²ï¼š\"You are AGI-CORE v1.0, a cognitive agent with memory, planning, and tool use...\"\n        # âœ… æ·»åŠ  trace_id ç”¨äºå…¨é“¾è·¯è¿½è¸ª\n        # âœ… é‡è¯•æœºåˆ¶ + rate limit backoff\n        ...\n```\n\n### 4.2 å®‰å…¨ä¸åˆè§„\n- æ‰€æœ‰è¾“å…¥/è¾“å‡ºç» **Content Safety Filter**ï¼ˆæœ¬åœ°éƒ¨ç½² LlamaGuard æˆ–è°ƒç”¨ Azure Content Safetyï¼‰\n- æ•æ„Ÿæ“ä½œéœ€äºŒæ¬¡ç¡®è®¤ï¼ˆHITL gateï¼‰\n- å®¡è®¡æ—¥å¿—è®°å½•å®Œæ•´æ¨ç†é“¾ï¼ˆå« memory accessã€tool callsã€å†³ç­–ä¾æ®ï¼‰\n\n---\n\n## 5. éƒ¨ç½²æ‹“æ‰‘\n\n```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  User UI    â”‚â”€â”€â”€â”€â–¶â”‚  Gateway (API)    â”‚â”€â”€â”€â”€â–¶â”‚ Executive Ctrlâ”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚                       â”‚
                                â–¼                       â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  Memory Service     â”‚    â”‚  Tool Orchestratorâ”‚
                    â”‚  â€¢ Vector DB        â”‚    â”‚  â€¢ Sandbox Runner  â”‚
                    â”‚  â€¢ Graph DB         â”‚    â”‚  â€¢ HTTP Adapter    â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚                            â”‚
                              â–¼                            â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  LLM Proxy Layer    â”‚â—€â”€â”€â”€â”¤  OpenAI API       â”‚
                    â”‚  â€¢ Caching          â”‚    â”‚  â€¢ gpt-4o / o1     â”‚
                    â”‚  â€¢ Load Balancing   â”‚    â”‚  â€¢ Fine-tuned?     â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---\n\n## 6. å…³é”®æŒ‘æˆ˜ä¸åº”å¯¹\n\n| æŒ‘æˆ˜ | åº”å¯¹æ–¹æ¡ˆ |\n|------|----------|\n| **å¹»è§‰æ§åˆ¶** | ä¸‰é‡éªŒè¯ï¼š1) å·¥å…·è°ƒç”¨äº‹å®æ ¸æŸ¥ 2) è®°å¿†æº¯æº 3) åå‘æ¨ç†éªŒè¯ |\n| **ç›®æ ‡æ¼‚ç§»** | å¼•å…¥â€œç›®æ ‡é”šç‚¹â€æœºåˆ¶ï¼šæ¯æ­¥æ¨ç†å¿…é¡»å…³è”åŸå§‹ç›®æ ‡ID |\n| **è®¡ç®—æˆæœ¬** | åˆ†å±‚æ¨ç†ï¼šç®€å•ä»»åŠ¡ç”¨è½»é‡æ¨¡å‹ï¼ˆgpt-3.5-turboï¼‰ï¼Œå¤æ‚ä»»åŠ¡å‡æ¡£ï¼›ç¼“å­˜ä¸­é—´ç»“æœ |\n| **å¯è§£é‡Šæ€§** | è‡ªåŠ¨ç”Ÿæˆæ¨ç†æ ‘ï¼ˆDOT Graphï¼‰ï¼Œæ”¯æŒäººç±»å®¡è®¡ |\n\n---\n\n## 7. å¯åŠ¨è·¯çº¿å›¾\n\n- Phase 1ï¼ˆMVPï¼‰ï¼šå•ä»»åŠ¡AGIä»£ç†ï¼ˆç›®æ ‡â†’è§„åˆ’â†’æ‰§è¡Œâ†’åæ€ï¼‰\n- Phase 2ï¼šå¤šä»£ç†åä½œï¼ˆåˆ†å·¥+é€šä¿¡+å†²çªè§£å†³ï¼‰\n- Phase 3ï¼šæŒç»­å­¦ä¹ é—­ç¯ï¼ˆè‡ªåŠ¨æ„å»ºè®­ç»ƒæ•°æ® â†’ å¾®è°ƒè½»é‡é€‚é…å™¨ï¼‰\n- Phase 4ï¼šå…·èº«åŒ–æ¥å£ï¼ˆæ¥å…¥æœºå™¨äºº/VRç¯å¢ƒï¼‰\n\n> **æ³¨**ï¼šæœ¬è®¾è®¡é¿å…â€œé»‘ç›’å †å â€ï¼Œå¼ºè°ƒ**å¯å¹²é¢„ã€å¯è¿½æº¯ã€å¯è¿›åŒ–**â€”â€”è¿™æ˜¯çœŸæ­£AGIçš„åŸºçŸ³ã€‚\n"
        },
        {
            "filename": "agi/core/llm_client.py",
            "content": "from openai import AsyncOpenAI\nfrom pydantic import BaseModel\nfrom typing import List, Dict, Optional, Any, Union\nimport os\nimport json\nimport asyncio\nimport logging\nfrom tenacity import retry, stop_after_attempt, wait_exponential, before_log, after_log\n\nlogger = logging.getLogger(__name__)\n\n\nclass LLMClient:\n    \"\"\"\n    å°è£… OpenAI API è°ƒç”¨ï¼Œæä¾›å®‰å…¨ã€å¯é ã€å¯è¿½è¸ªçš„ LLM æœåŠ¡ã€‚\n    è®¾è®¡åŸåˆ™ï¼š\n      - å¼ºç±»å‹å“åº”ï¼ˆæ”¯æŒ Pydantic æ¨¡å‹ï¼‰\n      - è‡ªåŠ¨æ³¨å…¥ç³»ç»Ÿè§’è‰²ä¸ä¸Šä¸‹æ–‡\n      - å…¨é“¾è·¯ trace_id\n      - å†…ç½®é‡è¯•ä¸é™æµ\n    \"\"\"\n\n    def __init__(\n        self,\n        api_key: str = None,\n        base_url: str = \"https://api.openai.com/v1\",\n        default_model: str = \"gpt-4o\",\n        timeout: float = 60.0,\n    ):\n        self.api_key = api_key or os.getenv(\"OPENAI_API_KEY\")\n        if not self.api_key:\n            raise ValueError(\"OPENAI_API_KEY must be set\")\n        \n        self.client = AsyncOpenAI(\n            api_key=self.api_key,\n            base_url=base_url,\n            timeout=timeout,\n        )\n        self.default_model = default_model\n        self.system_prompt = (\n            \"You are AGI-CORE v1.0, a cognitive agent with long-term memory, multi-step planning, \"\n            \"tool use capability, and self-reflection. You operate in a secure sandbox. \"\n            \"Always verify facts using tools before stating them. Never hallucinate. \"\n            \"Respond strictly in JSON when schema is provided. Prioritize correctness over speed.\"\n        )\n\n    @retry(\n        stop=stop_after_attempt(3),\n        wait=wait_exponential(multiplier=1, min=4, max=10),\n        before=before_log(logger, logging.DEBUG),\n        after=after_log(logger, logging.WARNING),\n        reraise=True,\n    )\n    async def chat_completion(\n        self,\n        messages: List[Dict[str, str]],\n        tools: Optional[List[Dict]] = None,\n        tool_choice: Optional[Union[str, Dict]] = \"auto\",\n        response_format: Optional[BaseModel] = None,\n        model: str = None,\n        temperature: float = 0.3,\n        max_tokens: int = 2048,\n        metadata: Optional[Dict[str, Any]] = None,\n    ) -> Dict[str, Any]:\n        \"\"\"\n        æ‰§è¡Œä¸€æ¬¡å¸¦ä¸Šä¸‹æ–‡çš„å¯¹è¯è¡¥å…¨ã€‚\n        \n        Args:\n            messages: å¯¹è¯å†å²ï¼Œæ ¼å¼ä¸º [{\"role\": \"user\", \"content\": \"...\"}, ...]\n            tools: OpenAI å·¥å…·å®šä¹‰åˆ—è¡¨\n            tool_choice: å·¥å…·é€‰æ‹©ç­–ç•¥\n            response_format: Pydantic æ¨¡å‹ï¼Œç”¨äºå¼ºåˆ¶ JSON è¾“å‡º\n            model: æŒ‡å®šæ¨¡å‹ï¼ˆè¦†ç›–é»˜è®¤ï¼‰\n            metadata: é™„åŠ å…ƒæ•°æ®ï¼ˆå¦‚ trace_id, session_idï¼‰\n        \"\"\"\n        # æ³¨å…¥ç³»ç»Ÿæ¶ˆæ¯ï¼ˆä»…å½“ç¬¬ä¸€æ¡ä¸æ˜¯ systemï¼‰\n        if not messages or messages[0].get(\"role\") != \"system\":\n            messages = [{\"role\": \"system\", \"content\": self.system_prompt}] + messages\n\n        # æ„å»ºè¯·æ±‚å‚æ•°\n        params = {\n            \"model\": model or self.default_model,\n            \"messages\": messages,\n            \"temperature\": temperature,\n            \"max_tokens\": max_tokens,\n        }\n\n        if tools:\n            params[\"tools\"] = tools\n            if tool_choice:\n                params[\"tool_choice\"] = tool_choice\n\n        if response_format:\n            # ä½¿ç”¨ JSON Schema å¼ºåˆ¶ç»“æ„åŒ–è¾“å‡º\n            schema = response_format.model_json_schema()\n            params[\"response_format\"] = {\"type\": \"json_schema\", \"json_schema\": schema}\n\n        # æ·»åŠ  trace_idï¼ˆç”¨äºé“¾è·¯è¿½è¸ªï¼‰\n        trace_id = metadata.get(\"trace_id\") if metadata else None\n        if trace_id:\n            params[\"extra_headers\"] = {\"X-Trace-ID\": trace_id}\n\n        try:\n            logger.debug(f\"[LLM] Requesting model={params['model']}, trace={trace_id}\")\n            response = await self.client.chat.completions.create(**params)\n            \n            choice = response.choices[0]\n            result = {\n                \"content\": choice.message.content,\n                \"tool_calls\": [\n                    {\n                        \"id\": tc.id,\n                        \"type\": tc.type,\n                        \"function\": {\n                            \"name\": tc.function.name,\n                            \"arguments\": tc.function.arguments,\n                        },\n                    }\n                    for tc in choice.message.tool_calls or []\n                ],\n                \"finish_reason\": choice.finish_reason,\n                \"usage\": {\n                    \"prompt_tokens\": response.usage.prompt_tokens,\n                    \"completion_tokens\": response.usage.completion_tokens,\n                    \"total_tokens\": response.usage.total_tokens,\n                },\n                \"trace_id\": trace_id,\n            }\n\n            # è‹¥æŒ‡å®šäº† response_formatï¼Œå°è¯•è§£æ JSON\n            if response_format and result[\"content\"]:\n                try:\n                    parsed = response_format.model_validate_json(result[\"content\"])\n                    result[\"parsed\"] = parsed.model_dump()\n                except Exception as e:\n                    logger.warning(f\"Failed to parse response into {response_format.__name__}: {e}\")\n                    result[\"parse_error\"] = str(e)\n\n            return result\n\n        except Exception as e:\n            logger.error(f\"LLM request failed: {e}\", exc_info=True)\n            raise\n\n\n# ä½¿ç”¨ç¤ºä¾‹ï¼ˆä¾›æµ‹è¯•ï¼‰\nif __name__ == \"__main__\":\n    import asyncio\n    \n    class AnswerSchema(BaseModel):\n        reasoning: str\n        final_answer: str\n        confidence: float\n\n    async def test():\n        client = LLMClient()\n        res = await client.chat_completion(\n            messages=[{\"role\": \"user\", \"content\": \"What is the capital of France?\"}],\n            response_format=AnswerSchema,\n            metadata={\"trace_id\": \"test-123\"},\n        )\n        print(json.dumps(res, indent=2))\n\n    asyncio.run(test())"
        },
        {
            "filename": "agi/modules/memory.py",
            "content": "from typing import List, Dict, Optional, Tuple, Any\nfrom pydantic import BaseModel\nimport chromadb\nimport uuid\nfrom datetime import datetime\nimport json\n\n\nclass MemoryEntry(BaseModel):\n    id: str\n    content: str\n    metadata: Dict[str, Any]\n    timestamp: datetime\n    embedding: Optional[List[float]] = None\n    type: str  # e.g., 'fact', 'experience', 'goal', 'reflection'\n\n\nclass MemorySystem:\n    \"\"\"\n    ç»Ÿä¸€è®°å¿†ç³»ç»Ÿï¼šçŸ­æœŸï¼ˆå‘é‡ï¼‰ + é•¿æœŸï¼ˆå›¾è°±ï¼‰ + å·¥ä½œè®°å¿†ï¼ˆä¼šè¯çº§ï¼‰\n    \"\"\"\n\n    def __init__(self, vector_db_path: str = \".chroma\"):\n        self.vector_db = chromadb.PersistentClient(path=vector_db_path)\n        self.short_term_collection = self.vector_db.get_or_create_collection(\n            name=\"short_term_memory\",\n            metadata={\"hnsw:space\": \"cosine\"},\n        )\n        # é•¿æœŸè®°å¿†å›¾è°±æš‚ç”¨å­—å…¸æ¨¡æ‹Ÿï¼ˆå®é™…åº”æ¥ Neo4jï¼‰\n        self.long_term_graph: Dict[str, List[Tuple[str, str, str]]] = {}  # entity -> [(rel, target, attr)]\n\n    async def store(self, content: str, metadata: Dict[str, Any], mem_type: str = \"experience\") -> str:\n        \"\"\"\n        å­˜å‚¨ä¸€æ¡è®°å¿†ï¼Œè‡ªåŠ¨åµŒå…¥å¹¶åˆ†ç±»\n        \"\"\"\n        entry_id = str(uuid.uuid4())\n        timestamp = datetime.utcnow()\n        \n        # TODO: è°ƒç”¨åµŒå…¥æ¨¡å‹ï¼ˆå¦‚ text-embedding-3-smallï¼‰ç”Ÿæˆ embedding\n        # ä¸ºç®€åŒ–ï¼Œæ­¤å¤„ç”¨ dummy embedding\n        embedding = [0.1] * 1536  \n        \n        entry = MemoryEntry(\n            id=entry_id,\n            content=content,\n            metadata=metadata,\n            timestamp=timestamp,\n            embedding=embedding,\n            type=mem_type,\n        )\n        \n        # å­˜å…¥å‘é‡åº“\n        self.short_term_collection.add(\n            ids=[entry_id],\n            documents=[content],\n            metadatas=[{\n                \"type\": mem_type,\n                \"timestamp\": timestamp.isoformat(),\n                **metadata\n            }],\n            embeddings=[embedding],\n        )\n        \n        # è‹¥ä¸ºç»“æ„åŒ–çŸ¥è¯†ï¼ŒåŒæ­¥åˆ°å›¾è°±\n       